# 写作大纲

1. 分布式并行都有哪些方案，对比？（参考熊猫的文章就行，主要对比 DP、TP、EP，以及 EP 的必要性）
3. torch 分布式的对比（pytorch mp 的文章和知乎的 ddp 的文章）
    * torch.mp.spwan
    * torchrun 
    * torch.distributed.launch
4. 代码
    1. alltoall kernel
    2. moe 训练
    3. 合并起来
    4. 分布式训练代码

# MoE并行都有哪些方案？他们的对比？

## DP TP 

## EP

**alltoall 是什么？**
EP 部署下，每个 GPU 持有若干个完整的专家。在 moe gating 计算完 token 和专家的亲和度之后，选取亲和度 topk 的专家。之后我们需要将 token 送到自己亲和（选中）的专家所在的 GPU 上【dispatch】，这个过程涉及到机内（甚至跨机）的 GPU 之间通信，逐渐成为 moe 推理的耗时瓶颈。

![torch_alltoall](img/image.png)

![AlltoAll 示意图](https://user-images.githubusercontent.com/8791375/72598353-a73c0580-3935-11ea-84e3-03c61f4d4935.png)


**为什么一定需要 alltoall？**
了解了是什么之后，为什么就一目了然了。EP 部署下，token 选择的专家不在当前的 GPU 上，则必须涉及到一个 token 路由 发送出去+接收回来 的操作。

# Torch 分布式的对比

## torch.distributed

## torch.multiprocessing.spawn()
