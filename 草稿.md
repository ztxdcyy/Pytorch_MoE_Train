# TODO
- [x] 需要用 git 管理代码，我需要把当前版本，训练失败的版本上传上去。
- [ ] debug记录

## Bug 1
MoE EP 单卡梯度断链排查总结

- 现象：main_ep.py 单卡/多卡训练中，gate 有梯度但所有专家梯度为 None/0，loss 震荡不收敛。
- 根因分析：all_to_all 路径打断计算图。单卡绕过通信时曾把 x 拷入 expert_x，破坏了对原输入的依赖；combine 中的通信写入也可能截断 autograd。
- 采取的修复：
1. 单卡分支绕过 all_to_all，直接按专家分桶引用 x 的切片，保持计算图。
2. combine 在 world_size==1 时改为本地 index_add 聚合，避免通信写和原地修改。
3. 打印 gate/专家梯度和带 grad 的专家数，便于确认链路。
- 后续验证建议：单卡模式应出现非零专家梯度并看到 loss 下降；若仍异常，再检查多卡通信的 autograd 支持或改用 torch.distributed.nn.functional.all_to_all。


### 设计思路

* 虽然是模拟，但是现在的本质分歧在于 EP 实现 or DP 实现。EP 需要额外实现 alltoall，而 DP 则仅仅是 ddp 包裹一下自定义的 model 即可。你深入对比下两种方案（我个人更倾向于 EP 但是EP 得写 alltoall 算子，可能更花时间一点，但是我喜欢做困难的事情！！）

EP-MoE（Expert Parallel + alltoall）

定下来 MoE 的配置：

```
    num_global_experts = 32
    world_size = 2/4
    num_local_experts = num_global_experts // world_size
    hidden_dim = 256
    experts_per_token = 2 (TopK=2 每个token选取2个专家)
    max_num_tokens = 128
```
我现在需要在draft里构建分布式环境，初始化分布式通信集群 init_process_group 并且 获得 rank

！！！ 我现在需要在 draft.py 里组织两个 alltoall kernel 需要的输入参数

dispatch 输入
```
dp_x：当前 rank（gpu）拥有的 token，shape = [num_tokens,hidden_dim]*
indices: 每个 token 选中的全局专家ID列表，形状 [num_tokens, experts_per_token]，值域 [0, num_experts)。
```

返回：
- expert_num_tokens：每个 local_expert 已经处理的 token 数量；
- expert_x：每个专家存储的 token 数据；expert_meta：与 expert_x 对齐的 metadata
```
expert_num_tokens,
expert_x, 
expert_meta
```

combine 需要
```
        out_tokens: torch.Tensor,  # output, (max num tokens, hidden_dim)
        weights: torch.Tensor,  # topk weight
        expert_meta: torch.Tensor,  # input
        expert_y: torch.Tensor,  # input, (num_local_experts, max_num_tokens * num_dp, hidden_dim)
        expert_num_tokens: torch.Tensor,
```

* 每个 token 的计算路径：
	1.	本地 gate 算出 top-k experts（是全局 expert index）；
	2.	根据 expert 的归属 rank，把 token feature 按目的地 rank 做一次 alltoall；
	3.	各 rank 只对自己本地 experts 做 GEMM（可 groupGEMM）；
	4.	再来一次 alltoall 把结果路由回原始 token 所在 rank，并按权重聚合。

👉 从“贴近真实系统”的角度看，EP 才是货真价实的 MoE 并行模型，DP 只是“所有卡各玩一遍同一套 MoE”

EP-MoE
*	真正的参数 sharding：专家参数在卡间切分；
	*	总 experts 数可以远大于“单卡能放下的 experts 数量”；
	*	world_size 越大，你能撑起的 experts 越多。
*	计算角度：
	*	每个 token 只在 token 路由到的 rank 计算一部分；
	*	全局 FLOPs 总量不变，但每卡只算它负责的 experts 对应那部分。
*	通信角度（关键）：
	*	两次 alltoall 的通信量 ≈ O(batch_size * hidden_dim * top_k / world_size) 定性理解
	*	experts 数越多 / top-k 越大 / batch 越大，通信成本就变得非常 relevant。
*	适合：
	*	想认真研究“通信-计算平衡”、“EP 的 scaling behavior”；
	*	想复现实用 MoE 架构（Mixtral / DeepSeek-MoE 这类）

### EP MoE 推理大致步骤
EP：你要实现一整条「路由 → pack → alltoall → local GEMM → alltoall → unpack」流水线

核心难点不在“调用 alltoall 这个 API”，而在于 如何优雅地组织数据，让它变成一个 / 几个大而规整的 tensor，既方便 alltoall，又方便 groupGEMM。

大致步骤：
1.	本地 gating（每 rank）
	*	同现在一样算出 topk_indices, topk_probs，但这里的 expert index 是全局的。
2.	构造 token-to-rank 映射
	*	你需要一个 mapping：`token i, kth expert -> dest_rank`
	*	从 expert_id 通过简单除法 / 查表得到 expert_owner_rank。
3.	按目的 rank 重排 / pack feature
	*	为每个目标 rank 建一个 “发给它的 token 列表”；
	*	把原始 x 按这些列表重排，拼成一个 [world_size, N_r, hidden] 或扁平的 [sum N_r, hidden]；
	*	为了让 alltoall 好做，你通常会：
	    *	每个 rank pad 到相同长度；
	    *	记录真实长度，用于解包。
4.	第一次 alltoall：把 token 发到专家所在 rank
	*	用 dist.all_to_all_single 把 [send_buf] 按 rank 交换；
	*	收到的是针对本 rank 所有本地 experts 需要处理的 tokens。
5.	本地按 expert 分组 + groupGEMM
	*	类似你现在的 per-expert 循环，但可以：
	*	排序让同一 expert 的 token 连在一起；
	*	==用 bmm / grouped GEMM 一次把多个 experts 的 matmul 干掉；【这里目前还不会】==
	*	输出还是 packed 格式。
6.	第二次 alltoall：把 expert 输出发回原始 token rank
	*	再来一个 alltoall，镜像 3 / 4 的逻辑，把结果送回去；
	*	解包，根据映射把结果加权累加回 [batch, hidden]。

你要处理的工程点包括：
	*	==稀疏路由导致的 tensor 尺寸不规则（需要 pad / pack）；【目前好像没有看到pad】==
	*	alltoall 的接口细节（all_to_all vs all_to_all_single，send/recv split_sizes 等）；
	*	debug 难度大幅上升，一不小心 rank 间的数据对不上就炸；

但反过来说：这正是“有技术含金量”的部分。
你搞出一个干净的 EP-MoE 实现，就是能写进“工程/系统向 PR 或简历”的那种东西。


![torch.dist.all_to_all_single](img/image.png)



## alltoall 怎么设计？

主要是设计合理的数据结构，一些 buffer 和 metadata。



## 怎么用？
eval 评估脚本，task 任务脚本，yml 配置文件包含测试用例

```
torchrun --nproc_per_node=8 /Users/tim/code/zomi/amd_distributed/eval.py --task-file /Users/tim/code/zomi/amd_distributed/all2all/task.yml
```

### worldsize 不区分单机和多机，这一点交给 dist.init_process_group 初始化分布式通信集群
world_size 在这份脚本里就是 EP 维度的全局进程数（多少个 rank 共同持有全部专家）。不区分单机/多机：只要分布式进程组已经用 dist.init_process_group 初始化好，all_to_all_single 能互通，脚本即可工作。

代码点：

* num_local_experts = cfg.num_experts // world_size：按全局 rank 数均分专家，每个 rank 持有自己的本地专家。
* dst_rank = e // num_local_experts：用全局专家 ID 映射到目标 rank，假设专家是顺序切块分配给 rank（0..world_size-1）。
* 所有通信用 dist.all_to_all_single，依赖后端（通常 NCCL）能覆盖全部 rank，所以可以是单机多卡，也可以是多机多卡，只要进程组连通。
* 约束：num_experts 应能整除 world_size，否则映射会错；多机时需要正确设置 MASTER_ADDR/MASTER_PORT/nnodes/node_rank 等，保证 rank 连通。

运行方式举例：

单机：torchrun --nproc_per_node=8 your_script.py
多机：torchrun --nnodes=2 --node_rank=0 --nproc_per_node=8 ...（另一节点 node_rank=1），确保环境变量指向同一 master。

## dispatch和combine的语义

* **dispatch 语义**：把本 rank 的输入 token (dp_x) 按路由 indices 分发到目标 rank 的本地专家。流程是先按目标 rank 分桶统计数量，然后 all-to-all 把 token 和元信息发出去，接收侧再按本地专家重排，输出 (expert_num_tokens, expert_x, expert_meta)：每个本地专家收到多少条、具体 token 数据、随行元信息。

* **combine 语义**：把各本地专家算好的输出 (expert_y) 按元信息送回原始 token 所在的 rank，并按 gating 权重累加。先按 expert_meta 中的源 rank 分桶，再 all-to-all 回传 token+元信息，接收侧用 src_token/src_k 找到原始 token 的位置和对应权重，写回 out_tokens[src_token]。之所以写回 src_token，是因为 MoE 的最终输出需要和输入 token 的顺序一一对应，每个 token 的多个专家贡献要聚合到它自己的槽位。


# main_ep.py

按照上面的讨论，在合适的位置创建一个main_ep.py脚本，并且填充以下内容：
初始化分布式环境，构造模型和优化器，准备测试数据，准备一个模拟的训练任务（拟合mlp，在 [draft.py](draft.py) 里 的train_tiny_moe_steps），传入moe配置（在 [config.py](my_moe/config.py)  里有抽象的类，在 [test_ep_moe.py](my_moe/test_ep_moe.py) 里有我写好的一个cfg，你直接复用就行）。如果有什么需要补充的，请及时和我讨论。


## 记录 

### 2025年12月11日

MoE EP 多卡梯度断链排查纪要

现象：单卡已能下降，Gate/专家都有梯度；多卡仍震荡，专家梯度为零或 None，任务 loss 不收敛。

已定位/修复的点：

combine 原地写导致叶子张量断图 → 改为 index_add 非原地。
单卡绕过通信时曾拷贝数据破坏计算图 → 改为直接切 x 的 slice 传给专家；单卡可正常下降。
清理了调试日志，恢复正常训练输出。
未解决的核心问题：多卡 all_to_all 路径仍断梯度。将通信换成 torch.distributed.nn.functional.all_to_all_single 仍未恢复梯度（至少在当前 CPU/gloo/Mac 环境下）。

建议的后续排查计划：

最小化梯度可用性测试：写一个两卡小脚本，x.requires_grad=True，经过一次 all_to_all_single、再 sum/backward，直接打印 x.grad，确认当前后端（gloo/CPU 或 NCCL/GPU）是否支持 backward。
如果后端不支持，在多卡路径上改为“手动 all_to_all”：先 all_gather counts，torch.split/torch.cat 重排，避免依赖通信的 autograd（代价是效率，但能验证问题所在）。
在有 NCCL/GPU 环境再试一次，排除 gloo/CPU 的限制；若 NCCL 下梯度恢复，则确认是后端支持问题。
保留最小的梯度监控（gate grad、任一专家 grad），便于快速判断链路是否恢复。
结论：单卡已验证链路正确，问题集中在多卡通信的 autograd 支持/实现上。下一步重点验证 all_to_all_single 在当前后端的反传支持，必要时用手工 gather/split 替代。