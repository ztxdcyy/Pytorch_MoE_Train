{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9249120-cfee-4534-849a-e2a675ab9e31",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<!--Copyright Â© ZOMI é€‚ç”¨äº[License](https://github.com/Infrasys-AI/AIInfra)ç‰ˆæƒè®¸å¯-->\n",
    "\n",
    "# MoE ä»åŸç†åˆ°åˆ†å¸ƒå¼å®ç°\n",
    "\n",
    "> Author by: å¼ å¤©ç¿”ã€ZOMI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f67bee-c6aa-455d-9ac5-6e0c7bc80a0f",
   "metadata": {},
   "source": [
    "åœ¨å‰é¢çš„å­¦ä¹ è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å·²ç»å®ç°äº†ä¸€ä¸ªå•æœºå•å¡MoEçš„å°demoï¼Œä½†æ˜¯å®é™…åœºæ™¯ä¸‹MoEæ¨¡å‹å¤ªå¤§ï¼ˆä¸“å®¶å¤§å°ï¼Œä¸“å®¶æ•°é‡ï¼‰ï¼Œå•GPUå†…å­˜ä¸èƒ½å®¹çº³æ‰€æœ‰ä¸“å®¶ï¼Œå› æ­¤è¦é‡‡å–åˆ†å¸ƒå¼éƒ¨ç½²çš„ç­–ç•¥æ‰èƒ½è®©ä¸“å®¶æƒé‡å­˜å‚¨åœ¨GPUæ˜‚è´µçš„å†…å­˜ä¸­ã€‚é‚£ä¹ˆé—®é¢˜æ¥äº†ï¼Œå·²çŸ¥æœ‰DPã€TPã€EPç­‰å¤šç§å¹¶è¡Œæ–¹æ¡ˆï¼Œå“ªä¸€ç§æ–¹æ¡ˆæ›´åŠ è´´åˆMoEæ¨¡å‹ï¼ˆç®—æ³•ä¾§ï¼‰å‘¢ï¼Ÿä¸ºä»€ä¹ˆå‘¢ï¼Ÿè¿™äº›å¹¶è¡Œæ–¹æ¡ˆä¹‹é—´å­°ä¼˜å­°åŠ£ï¼Œå„è‡ªé€‚åˆä»€ä¹ˆåœºæ™¯å‘¢ï¼Ÿæœ¬æ–‡å°±è®©æˆ‘ä»¬ä¸€èµ·æ¥æ¢ç©¶åŸç†ï¼Œæœ€ååŠ¨æ‰‹å®ç°ä¸€ä¸ªåˆ†å¸ƒå¼MoEï¼ˆEPï¼‰çš„åˆ†å¸ƒå¼demoã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4ab85d-2096-406b-81bd-c4237529e1dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# åŸç†åˆ†æ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc8cdcc-14f0-42b0-8afa-aa2ac527c276",
   "metadata": {},
   "source": [
    "## DPã€TPã€EPå®šä¹‰\n",
    "é¦–å…ˆå›ç­”â€œæ˜¯ä»€ä¹ˆâ€ï¼Œå‚è€ƒäº†[è¿™ç¯‡æ–‡ç« ](https://zhuanlan.zhihu.com/p/1967192540953425479)ï¼š\n",
    "* DPï¼šæ¯å—GPUå¤åˆ¶å…¨é‡æ¨¡å‹ï¼Œæ‹†åˆ†è¾“å…¥batchç»™æ¯ä¸ªè®¾å¤‡ç‹¬ç«‹è®¡ç®—ï¼Œæ¯ä¸ªè®¾å¤‡éƒ½èƒ½ç‹¬ç«‹å¾—åˆ°ç»“æœï¼Œä¸éœ€è¦é€šä¿¡ã€‚\n",
    "* TPï¼šå°†æ¨¡å‹æƒé‡åˆ‡åˆ†æˆè‹¥å¹²chunksï¼Œå°†å°chunkæ”¾åœ¨å•ä¸ªGPUä¸Šã€‚ç”±äºæ¨¡å‹è¢«åˆ‡åˆ†ï¼Œä¸èƒ½ç‹¬ç«‹å¾—åˆ°ç»“æœï¼Œæ‰€ä»¥éœ€è¦AllReduceé€šä¿¡ã€‚å‚è€ƒè¿™ç¯‡[æ–‡ç« ](https://zhuanlan.zhihu.com/p/622212228)ï¼Œå¾ˆæ¨èé˜…è¯»ã€‚\n",
    "    * æ¨¡å‹åˆ‡åˆ†æœ‰ä¸¤ç§æ–¹å¼ï¼šæ¨ªç€åˆ‡å’Œç«–ç€åˆ‡ï¼›è¿™é‡Œæœ€å¥½ææ˜ç™½â€”â€”MLPå…ˆåˆ—åˆ‡å†è¡Œåˆ‡ï¼Œç”±äºGELUçš„æ€§è´¨å¯ä»¥å‡å°‘ä¸€æ¬¡ä¸å¿…è¦çš„allreduceé€šä¿¡![](./images/Practice03DistMoE01.png)\n",
    "    * Transformeræ¶æ„ä¸­å¤§æ¦‚æœ‰ä¸¤å—å¯ä»¥ç”¨TPåšåˆ‡åˆ†ï¼šMLPå’ŒMHAï¼ŒMLPåˆ‡åˆ†å°±åœ¨ä¸Šå›¾ï¼ŒMLAåˆ‡åˆ†å°±æ²¿ç€num_headsç»´åº¦åˆ‡åˆ†QKVweightsåˆ°å¤šä¸ªGPUsä¸Šã€‚\n",
    "* EPï¼šé’ˆå¯¹MoEè¿™ä¸€å±‚ï¼Œå°†ä¸“å®¶æƒé‡ï¼ˆMLPï¼‰ç‹¬ç«‹çš„æ”¾åœ¨ä¸åŒçš„GPUä¸Šã€‚æ¯”å¦‚æœ‰32ä¸ªä¸“å®¶å’Œ4å—GPUï¼Œåˆ™æ¯ä¸ªGPUå®¹çº³8ä¸ªç‹¬ç«‹çš„ä¸“å®¶æƒé‡ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9994a0-4458-455a-9f96-8cb4e07c826d",
   "metadata": {},
   "source": [
    "## EP AlltoAll \n",
    "\n",
    "EPéƒ¨ç½²å¸¦æ¥çš„ä¸€ä¸ªæ ¸å¿ƒé—®é¢˜æ˜¯ï¼Œä¸“å®¶ä¸å†åœ¨åŒä¸€ä¸ªGPUä¸Šäº†ï¼Œè€Œè¾“å…¥batch-sizeä¸ªtokensï¼Œå®ƒä»¬æ ¹æ®gateç½‘ç»œæ‰“åˆ†ï¼Œé€‰æ‹©è¯¥tokenè¦å‰å¾€çš„ä¸“å®¶ï¼›åœ¨é€‰ä¸­ä¸“å®¶ä¸Šè®¡ç®—å®Œä¹‹åéœ€è¦é‡æ–°å›åˆ°å®ƒæ¥çš„é‚£å¼ å¡ä¸Šï¼Œè¿™ä¸ªè¿‡ç¨‹å°±ç§°ä¸ºAlltoAllï¼ˆç®€ç§°ataï¼‰é€šä¿¡ã€‚\n",
    "\n",
    "é…åˆå›¾ç‰‡ç›´è§‚çš„ç†è§£ä¸€ä¸‹ï¼Œå›¾ç‰‡æ¥æºï¼šhttps://zhuanlan.zhihu.com/p/28867733102\n",
    "\n",
    "![dispatch_visualiaze](./images/Practice03DistMoE02.png)\n",
    "\n",
    "æœ‰å››ä¸ªdeviceï¼Œæ¯ä¸ªGPUè¾“å…¥ä¸¤ä¸ªtokenï¼Œæ¯ä¸ªtokené€‰æ‹©å››ä¸ªä¸“å®¶ï¼ˆå››ä¸ªç®­å¤´ï¼‰ã€‚é¢œè‰²ä»£è¡¨ä¸“å®¶ï¼Œæœ‰å…«ç§é¢œè‰²æ„å‘³ç€æœ‰å…«ä¸ªä¸“å®¶ï¼Œæ¯å—GPUä¸Šæœ‰ä¸¤ä¸ªä¸“å®¶ï¼ˆæƒé‡ï¼‰ã€‚tokenæœ€å¼€å§‹æ˜¯æ²¡æœ‰é¢œè‰²çš„ï¼ˆç°è‰²ï¼Œä»£è¡¨è¿˜æ²¡é€‰æ‹©ä¸“å®¶ï¼‰ã€‚\n",
    "\n",
    "æ‹¿device1ä¸Šçš„token2ä¸¾ä¾‹ï¼Œæœ‰å››ä¸ªç®­å¤´ä»£è¡¨é€‰ä¸­äº†å››ä¸ªä¸“å®¶ã€‚permuteè¿™æ­¥ä¹‹åä»£è¡¨åšå®Œäº†ä¸“å®¶é€‰æ‹©ï¼ˆtokenå¼€å§‹æœ‰é¢œè‰²ï¼Œé¢œè‰²ä»£è¡¨äº†è¦å»çš„GPUâ€œç»„â€ï¼Œè¿™é‡Œä¸€ä¸ªGPUä¸Šæœ‰ä¸¤ç§é¢œè‰²ï¼Œä¹Ÿå°±æ˜¯ä¸¤â€œç»„â€ï¼‰ã€‚alltoallè¿™æ­¥ä¹‹åï¼Œæ¯ä¸ªtokenå»åˆ°é¢œè‰²å¯¹åº”çš„ä¸“å®¶ã€‚æœ€åsort_chunksby_idxsï¼ŒåŒä¸€å—GPUå†…æœ‰ä¸¤ç§é¢œè‰²ï¼Œæ¯ä¸ªé¢œè‰²å†…æ ¹æ®è¾“å…¥tokenidå¤§å°åšå‡åºã€‚\n",
    "\n",
    "ataåœ¨NCCLæ–‡æ¡£ä¸­æ˜¯è¿™ä¹ˆå®šä¹‰çš„ï¼šæ¯ä¸ªè¿›ç¨‹å‘æ‰€æœ‰å…¶ä»–è¿›ç¨‹å‘é€countä¸ªå€¼ï¼Œå¹¶ä»æ‰€æœ‰å…¶ä»–è¿›ç¨‹æ¥æ”¶countä¸ªå€¼ã€‚å‘é€åˆ°ç›®æ ‡è¿›ç¨‹jçš„æ•°æ®å–è‡ªsendbuff+j\\*countï¼Œä»æºè¿›ç¨‹iæ¥æ”¶çš„æ•°æ®è¢«æ”¾ç½®åœ¨recvbuff+i\\*countå¤„ã€‚å¯¹åº”ç€ä¸‹é¢è¿™å¼ å›¾ï¼š\n",
    "\n",
    "\n",
    "![NCCL_AlltoAll](./images/Practice03DistMoE03.png)\n",
    "\n",
    "\n",
    "è€Œå®ƒåœ¨Pytorchä¸­æœ‰è¿™æ ·ä¸€ä¸ªæ¥å£ï¼Œå‡½æ•°ç­¾åï¼š`torch.distributed.all_to_all_single(output, input, output_split_sizes=None, input_split_sizes=None, group=None, async_op=False)`\n",
    "å°†è¾“å…¥å¼ é‡æ‹†åˆ†ï¼Œç„¶åå°†æ‹†åˆ†åçš„åˆ—è¡¨åˆ†æ•£åˆ°ç»„ä¸­çš„æ‰€æœ‰è¿›ç¨‹ã€‚ä¹‹åï¼Œä»ç»„ä¸­æ‰€æœ‰è¿›ç¨‹æ¥æ”¶çš„å¼ é‡ä¼šè¢«è¿æ¥èµ·æ¥ï¼Œå¹¶ä½œä¸ºå•ä¸ªè¾“å‡ºå¼ é‡è¿”å›ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcacbe2",
   "metadata": {},
   "source": [
    "## ä¸ºä»€ä¹ˆå¤§è§„æ¨¡éƒ¨ç½²ä¼˜å…ˆé€‰æ‹©EPï¼Ÿ\n",
    "\n",
    "å®Œå…¨ï¼ˆMoE+non-MoEï¼‰ç”¨DPéƒ¨ç½²ç”±äºå†…å­˜åŸå› é¦–å…ˆè¢«æ’é™¤ï¼Œæˆ‘ä»¬è¯•ç€ä»è®¡ç®—ã€é€šä¿¡ä¸¤ä¸ªè§’åº¦å»åˆ†æTPã€EPå­°ä¼˜å­°åŠ£ï¼Ÿ\n",
    "\n",
    "### é€šä¿¡é‡åˆ†æ\n",
    "\n",
    "ç»™å‡ºæŠ½è±¡çš„å®šé‡åˆ†æï¼šmoe_config:{num_experts, ep_world_size(rank_size), topk}, local_token.shape = [b, s, h]\n",
    "\n",
    "åˆå§‹ï¼Œæ¯ä¸ªrankæœ‰$b*s$ä¸ªtokenï¼›dispatchå®Œæˆä¹‹åï¼Œæ¯ä¸ªrankæœ‰$b*s*topk$ä¸ªtokenï¼ˆå‡è®¾ä¸“å®¶è´Ÿè½½å‡è¡¡ï¼‰ã€‚æˆ‘ä»¬ç°åœ¨æƒ³æ±‚å‡ºéœ€è¦çœŸæ­£å‘é€å‡ºå»çš„tokenæ•°é‡ï¼ˆç»ˆç‚¹dstä¸åœ¨æœ¬rankä¸Šï¼‰ï¼Œæ¯ä¸ªrankå‘é€$b*s*topk$ä¸ªtokenï¼Œè¿™ä¸ªtokenè½åœ¨å…¶ä»–rankçš„æ¦‚ç‡æ˜¯$\\frac{ep\\_world\\_size-1}{ep\\_world\\_size}$ï¼Œæ‰€ä»¥å¯¹æœ¬rankçœŸæ­£éœ€è¦è·¨å¡å‘é€å‡ºå»çš„tokenæ•°é‡çš„æœŸæœ›æ˜¯$b*s*topk* \\frac{ep\\_world\\_size-1}{ep\\_world\\_size}$ã€‚åŠç²¾åº¦æƒ…å†µä¸‹ï¼Œæ•°æ®é‡æ˜¯$2*b*s*topk* \\frac{ep\\_world\\_size-1}{ep\\_world\\_size}*h$ bytesã€‚åŒæ—¶ï¼Œcombineæ˜¯dispatchçš„é€†è¿‡ç¨‹ï¼Œå› æ­¤å¯¹äºæ¯ä¸ªrankæ¥è¯´ï¼ŒåŠç²¾åº¦ï¼Œå®Œæ•´ep ataé€šä¿¡é‡è¿‘ä¼¼ä¸º$4*b*s*h*topk$ Bytesã€‚\n",
    "\n",
    "è€ŒTPï¼Œåœ¨ä¸Šé¢çš„å›¾é‡Œï¼ŒZ1Z2éƒ½æ˜¯$V = [b,s,h]$ï¼Œéœ€è¦åšä¸€æ¬¡allreduceå¾—åˆ°æœ€åçš„Zï¼ŒåŒæ ·å¯¹äºåŠç²¾åº¦ï¼Œæ¯ä¸ªtp-ranké€šä¿¡çš„æ•°æ®é‡æ˜¯$2*b*s*h*2 = 4*b*s*h$ bytesã€‚ç¬¬ä¸€ä¸ª2å› ä¸ºåŠç²¾åº¦ï¼Œç¬¬äºŒä¸ª2å› ä¸ºring-allreduceåŒ…å«äº†reduce scatterå’Œallgatherä¸¤ä¸ªæ­¥éª¤ã€‚å‡è®¾TPsize=Tï¼Œæ¯ä¸ªrankéœ€è¦è¿›è¡Œé€šä¿¡çš„tensorå¤§å°æ˜¯Vï¼Œring-allreduceä¼šå°†Våˆ‡æˆ$\\frac{V}{T}$å¤§å°çš„chunkã€‚åœ¨reduce scatteré˜¶æ®µï¼Œæ¯ä¸ªstepä¸‹æ¯ä¸ªrankå‘é€$\\frac{V}{T}$çš„chunkï¼Œæ€»å…±æœ‰$T-1$æ­¥ï¼Œå› æ­¤åœ¨reduce scatteré˜¶æ®µï¼Œæ¯ä¸ªrankå‘é€äº†$\\frac{T-1}{T}V$æ•°æ®ã€‚allgatheré˜¶æ®µåŒç†ï¼Œå› æ­¤å¯¹äºæ¯ä¸ªtp rankï¼Œring-allreduceé€šä¿¡é‡æ˜¯$2 \\times \\frac{T-1}{T}V$ï¼Œåœ¨åŠç²¾åº¦æƒ…å†µä¸‹ï¼Œçº¦ç­‰äº$4*b*s*h*$ bytesã€‚\n",
    "\n",
    "ï¼ˆä¸Šé¢çš„åˆ†æä¸­ï¼ŒTPå’ŒEPéƒ½è¿‘ä¼¼æ‰äº†$\\frac{T-1}{T}$è¿™ä¸ªç³»æ•°ï¼Œæ‰€ä»¥å¾—åˆ°çš„ç»“è®ºæ˜¯ç²¾ç¡®çš„ï¼‰çœ‹èµ·æ¥TPæ¯”EPä¼˜åŒ–äº†topkå€çš„é€šä¿¡é‡ï¼ŒçœŸçš„æ˜¯è¿™æ ·å—ï¼Ÿ\n",
    "\n",
    "å®é™…éƒ¨ç½²ä¸‹ï¼Œè¿˜éœ€è¦è€ƒè™‘DPçš„å› ç´ ã€‚å‡è®¾æ€»tokenæ•°é‡æ˜¯$1*128$ï¼Œworld_size=32ï¼ˆgpuæ•°é‡ï¼Œ4å¼ 8å¡çš„æœºå™¨ï¼‰ã€‚\n",
    "* å¯¹äºTP=8æ¥è¯´ï¼ŒDP=4ï¼Œé¦–å…ˆå››å°æœºå™¨ï¼Œæ¯ä¸ªæœºå™¨è¾“å…¥$128 \\div 4 =32$ä¸ªtokenï¼Œå¯¹äºæ¯ä¸ªtprankæ¥è¯´ï¼Œé€šä¿¡é‡æ˜¯$4*1*32*h=128h$bytesã€‚\n",
    "* ä½†æ˜¯å¯¹äºEP=8æ¥è¯´ï¼Œå¼€EPå¹¶ä¸å½±å“DPæ•°é‡ã€‚åªæœ‰moeå±‚é€šè¿‡è·¨æœºEP ataè¿›è¡Œé€šä¿¡ï¼Œè€Œémoeå±‚å¯ä»¥å¼€DP=32ï¼Œæƒé‡å®Œå…¨å¤åˆ¶ï¼Œå†…å­˜å‹åŠ›ç¨å¤§ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæ¯å¼ å¡è¾“å…¥$128 \\div 32 =4$ä¸ªtokenï¼Œåœ¨gatingè®¡ç®—å®Œä¹‹åï¼Œtokené€šè¿‡ataè¿›è¡Œä¸€æ¬¡æœºå†…é€šä¿¡ï¼ˆå› ä¸ºEP=8ï¼Œè€Œä¸€ä¸ªèŠ‚ç‚¹å…«å¼ å¡ï¼Œæ­£å¥½ä¸éœ€è¦è·¨æœºé€šä¿¡ï¼‰ï¼Œåœ¨dstå¡è®¡ç®—å®Œåè¿”å›åˆ°srcå¡ã€‚é€šä¿¡é‡æ˜¯$4*1*4*h*topk=16h*topk$ bytesã€‚\n",
    "* æ­¤æ—¶å†å¯¹æ¯”TPã€EPé€šä¿¡é‡ï¼Œå½“$topk>8$æ—¶ï¼ŒEPé€šä¿¡é‡å¤§äºTPï¼Œå¦åˆ™EPæ›´ä¼˜ã€‚äº‹å®ä¸Šï¼ŒæŠ½è±¡ä¸€ä¸‹å°±å¯ä»¥çŸ¥é“ï¼Œè¿™é‡Œçš„8æ­£æ˜¯parallelsize=tpsize=epsizeã€‚\n",
    "* å¾—å‡ºç»“è®ºï¼Œå½“$top\\_k<parallel\\_size$æ—¶ï¼ŒEPæ›´ä¼˜ã€‚\n",
    "\n",
    "### è®¡ç®—ä¼˜åŠ¿åˆ†æ\n",
    "è®¡ç®—ä¸Šçš„ä¼˜åŠ¿ä¸»è¦æ¥æºäºEPå¯ä»¥åœ¨æœºå†…å°†å¤šä¸ªæœ¬åœ°ä¸“å®¶è®¡ç®—èšåˆä¸ºå•ä¸ªæ‰¹å¤„ç†GEMM(batched-gemm, bmm)æ“ä½œã€‚è¿™æ ·æˆ‘ä»¬å°±ä¸ç”¨é€šè¿‡`for expert in range(num_experts):`è¿™æ ·çš„å¾ªç¯ï¼Œé™ä½äº†é¢‘ç¹kernel launchçš„å¼€é”€ï¼›é™¤æ­¤ä»¥å¤–ï¼Œç”±äºGPUé’ˆå¯¹å¤§è§„æ¨¡å¹¶è¡Œè®¡ç®—è¿›è¡Œäº†ä¼˜åŒ–ï¼Œå°†å¤šä¸ªå°å°ºå¯¸çŸ©é˜µä¹˜æ³•èšåˆæˆå•ä¸ªå¤§å°ºå¯¸çŸ©é˜µä¹˜æ³•çš„æ“ä½œèƒ½æ›´å¥½åœ°å‘æŒ¥å…¶æ€§èƒ½ï¼Œä»è€Œæé«˜åˆ©ç”¨ç‡å’Œæ•ˆç‡ã€‚\n",
    "\n",
    "å…·ä½“å¯ä»¥å»çœ‹æˆ‘ç¿»è¯‘çš„è¿™ç¯‡åšå®¢ï¼š[What Shapes Do Matrix Multiplications Like?](https://zhuanlan.zhihu.com/p/1984323735117919137)ï¼Œä»¥åŠ Pytorch Blog â€”â€” [Training MoEs at Scale with PyTorch](https://pytorch.org/blog/training-moes/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2ef2e7",
   "metadata": {},
   "source": [
    "# ä»£ç å®ç°\n",
    "\n",
    "è¿™ä¸€èŠ‚æŠŠå‰é¢çš„åˆ†æè½åˆ°ä»£ç ï¼šæˆ‘ä»¬å®ç°ä¸€ä¸ª **Expert-Parallelï¼ˆEPï¼‰MoE layer** çš„æœ€å°å¯è¿è¡Œç‰ˆæœ¬ï¼Œå¹¶ç”¨ä¸€ä¸ª toy ä»»åŠ¡è·‘é€šè®­ç»ƒé—­ç¯ï¼Œé‡ç‚¹æ”¾åœ¨ï¼š\n",
    "\n",
    "1. **routing / top-k**ï¼šgate å¯¹æ¯ä¸ª token é€‰æ‹© top-k ä¸“å®¶ï¼›\n",
    "2. **dispatchï¼ˆAll-to-Allï¼‰**ï¼šæŠŠ token æ´¾å‘åˆ°â€œä¸“å®¶æ‰€åœ¨çš„ rankâ€ï¼›\n",
    "3. **local expert forward**ï¼šæ¯ä¸ª rank åªè®¡ç®—è‡ªå·±æŒæœ‰çš„æœ¬åœ°ä¸“å®¶ï¼›\n",
    "4. **combineï¼ˆAll-to-Allï¼‰**ï¼šæŠŠä¸“å®¶è¾“å‡ºå›ä¼ åˆ°æº rankï¼Œå¹¶æŒ‰ top-k æƒé‡èšåˆå› token åºåˆ—ï¼›\n",
    "5. **è®­ç»ƒé—­ç¯**ï¼šåå‘ä¼ æ’­ç©¿è¿‡ gate + experts + é€šä¿¡ç®—å­ï¼ˆè¿™é‡Œç”¨ autograd-aware çš„ dist.all_gather å®ç°ï¼‰ã€‚\n",
    "\n",
    "å®ç°é‡Œ ataï¼ˆall-to-allï¼‰ç®—å­å‚è€ƒäº†è¿™ä¸ª [reference](https://github.com/gpu-mode/reference-kernels/blob/eff169759596326890b23d4625cb6d5923266e55/problems/amd_distributed/all2all/reference.py#L54)\n",
    "\n",
    "è¯´æ˜ï¼šå®Œæ•´çš„é«˜æ€§èƒ½ MoE/EP è®­ç»ƒæ˜¾ç„¶æœ‰æ›´å¤šä¼˜åŒ–ç©ºé—´ï¼Œä¾‹å¦‚ï¼š\n",
    "* æ›´ç³»ç»Ÿçš„åˆ†å¸ƒå¼å¼ é‡/æ¢¯åº¦ç®¡ç†æˆ–è€…å¹¶è¡Œï¼ˆæ¯”å¦‚ DTensorï¼‰\n",
    "* è®¡ç®—é€šä¿¡å¹¶è¡Œæ©ç›–é€šä¿¡å¼€é”€ï¼šTBOä½¿ç”¨åŒbatché‡å æ¥æ©ç›–å¤šæœºå¤šå¡Ataé€šä¿¡å¼€é”€ï¼›Deepseekæå‡ºçš„Shared Expertsçš„è®¡ç®—ä¹Ÿå¯ä»¥å’Œdispatché€šä¿¡å¹¶è¡Œé‡å ã€‚\n",
    "* é«˜æ•ˆé€šä¿¡ï¼šataé€šä¿¡ä¸­å­˜åœ¨è¾ƒå¤§çš„é€šä¿¡å†—ä½™ï¼Œæ‹¿æˆ‘ä»¬æœ€ä¸Šé¢çš„é‚£å¼ å›¾æ¥è¯´æ˜ï¼š\n",
    "    * åŒä¸€ä¸ª token dispatch åˆ°åŒä¸€å° device çš„ä¸åŒ expert ï¼ˆå¦‚ token2 dispatch åˆ° device 2 çš„ ä¸¤ä¸ª expert)ã€‚\n",
    "    * åŒä¸€ä¸ª token dispatch åˆ°åŒä¸€å° host çš„ ä¸åŒ device (å¦‚ token2 dispatch åˆ° device 3 å’Œ device 4)ã€‚è¿™ç‚¹å¯ä»¥ç”±ä¸€æ¬¡æœºé—´é€šä¿¡åŠ æœºå†…é€šä¿¡æ›¿ä»£æ¥å‡å°‘æœºé—´é€šä¿¡ã€‚\n",
    "\n",
    "åœ¨è¿™ä¸ªNotebooké‡Œï¼Œæˆ‘ä»¬ä»¥â€œå¯è¯» + å¯è·‘é€šâ€ä¸ºç›®æ ‡ï¼Œå…ˆæŠŠæ ¸å¿ƒæ•°æ®æµå’Œé€šä¿¡é€»è¾‘å†™æ¸…æ¥šã€‚\n",
    "\n",
    "> å°æç¤ºï¼šæœ¬ notebook æœ€åä¸€ä¸ª cell ä¼šç”¨ `torchrun` å¯åŠ¨å¤šè¿›ç¨‹è®­ç»ƒã€‚å› ä¸º `torchrun` æ˜¯æ–°è¿›ç¨‹ï¼Œå®ƒä¸ä¼šå…±äº« notebook å†…å­˜ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¼šä» notebook ä¸­æŠ½å–è‹¥å¹² code cell æ‹¼æˆä¸´æ—¶è„šæœ¬å†è¿è¡Œã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eae9ed5-e45e-4d16-b222-fda594912e45",
   "metadata": {},
   "source": [
    "## ç¯å¢ƒæ£€æŸ¥\n",
    "\n",
    "ç¯å¢ƒæœ‰å¤šé‡è¦æƒ³å¿…ä¸å¿…å¤šè¯´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c1f0e12-a579-4467-b270-5276f10f4f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ£€æµ‹åˆ° 2 ä¸ª GPU\n",
      "âœ… å¤š GPU ç¯å¢ƒï¼Œå°†ä½¿ç”¨ torchrun å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ (å»ºè®®ä½¿ç”¨ 2 ä¸ª GPU)\n",
      "ğŸ“ åç»­å®éªŒå°†é€šè¿‡ %%writefile åˆ›å»ºä¸´æ—¶è„šæœ¬ï¼Œè‡ªåŠ¨è¿è¡Œ torchrunï¼Œå¹¶æ¸…ç†ä¸´æ—¶æ–‡ä»¶\n",
      "\n",
      " å®éªŒé…ç½®:\n",
      "  - GPU æ•°é‡: 2\n",
      "  - CUDA å¯ç”¨: True\n",
      "  - PyTorch ç‰ˆæœ¬: 2.3.0+cu121\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# æ£€æµ‹ GPU æ•°é‡\n",
    "gpu_count = torch.cuda.device_count()\n",
    "print(f\"æ£€æµ‹åˆ° {gpu_count} ä¸ª GPU\")\n",
    "\n",
    "if gpu_count >= 2:\n",
    "    print(f\"âœ… å¤š GPU ç¯å¢ƒï¼Œå°†ä½¿ç”¨ torchrun å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ (å»ºè®®ä½¿ç”¨ {gpu_count} ä¸ª GPU)\")\n",
    "    print(\"ğŸ“ åç»­å®éªŒå°†é€šè¿‡ %%writefile åˆ›å»ºä¸´æ—¶è„šæœ¬ï¼Œè‡ªåŠ¨è¿è¡Œ torchrunï¼Œå¹¶æ¸…ç†ä¸´æ—¶æ–‡ä»¶\")\n",
    "else:\n",
    "    print(\"âš ï¸  è­¦å‘Š: æ£€æµ‹åˆ°å°‘äº 2 ä¸ª GPUï¼Œåˆ†å¸ƒå¼è®­ç»ƒå¯èƒ½æ— æ³•æ­£å¸¸è¿è¡Œ\")\n",
    "\n",
    "print(f\"\\n å®éªŒé…ç½®:\")\n",
    "print(f\"  - GPU æ•°é‡: {gpu_count}\")\n",
    "print(f\"  - CUDA å¯ç”¨: {torch.cuda.is_available()}\")\n",
    "print(f\"  - PyTorch ç‰ˆæœ¬: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeb4d88",
   "metadata": {},
   "source": [
    "## åŸºæœ¬ç»„ä»¶å®ç°\n",
    "\n",
    "### class MoEConfig\n",
    "\n",
    "å…ˆå®šä¹‰ä¸€ä¸ª `MoEConfig`ï¼Œç»Ÿä¸€ç®¡ç† EP-MoE é‡Œæœ€å…³é”®çš„è¶…å‚æ•°ï¼š\n",
    "\n",
    "- `num_experts`ï¼šå…¨å±€ä¸“å®¶æ€»æ•°ï¼ˆæ‰€æœ‰ rank ä¸Šä¸“å®¶çš„æ€»å’Œï¼‰ã€‚\n",
    "- `experts_per_token`ï¼šæ¯ä¸ª token é€‰æ‹©çš„ä¸“å®¶ä¸ªæ•°ï¼ˆtop-kï¼‰ã€‚\n",
    "- `hidden_dim`ï¼štoken hidden sizeï¼Œä¹Ÿæ˜¯ä¸“å®¶ MLP çš„è¾“å…¥/è¾“å‡ºç»´åº¦ã€‚\n",
    "- `max_num_tokens`ï¼šç”¨äºé€šä¿¡ buffer çš„ä¸Šç•Œï¼ˆéœ€è¦èƒ½è¦†ç›–æœ¬ rank ä¸€æ¬¡å‰å‘é‡Œå¯èƒ½å‡ºç°çš„ token æ•°ï¼‰ã€‚\n",
    "- `in_dtype/out_dtype`ï¼šè¾“å…¥/è¾“å‡º dtypeï¼ˆå®ç°é‡Œä¼šåœ¨ä¸“å®¶è®¡ç®—æ—¶ä¸´æ—¶è½¬æˆ `float32` ä»¥ç®€åŒ–æ•°å€¼é—®é¢˜ï¼‰ã€‚\n",
    "\n",
    "çº¦æŸ/æ³¨æ„ï¼š\n",
    "- è®­ç»ƒæ—¶å¦‚æœ `bsz`ï¼ˆæˆ– token æ•°ï¼‰è¶…è¿‡ `max_num_tokens`ï¼Œéœ€è¦å¢å¤§ `max_num_tokens`ï¼Œå¦åˆ™åé¢ä¼šå‡ºç° buffer ä¸å¤Ÿçš„æƒ…å†µã€‚\n",
    "- `num_experts` å¿…é¡»èƒ½è¢« `world_size` æ•´é™¤ï¼ˆæ¯ä¸ª rank æ‹¥æœ‰åŒæ ·æ•°é‡çš„æœ¬åœ°ä¸“å®¶ï¼‰ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0403552a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "moe_train"
    ]
   },
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import torch\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class MoEConfig:\n",
    "    num_experts: int\n",
    "    experts_per_token: int\n",
    "    hidden_dim: int\n",
    "    max_num_tokens: int\n",
    "    in_dtype: torch.dtype = torch.float16\n",
    "    out_dtype: torch.dtype = torch.float16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97de96c",
   "metadata": {},
   "source": [
    "### class PyTorchAllToAll\n",
    "\n",
    "è¿™éƒ¨åˆ†å®ç° EP æœ€æ ¸å¿ƒçš„é€šä¿¡ç®—å­ï¼ˆata / all-to-allï¼‰çš„ä¸€ä¸ªâ€œæ•™å­¦ç‰ˆâ€ã€‚\n",
    "\n",
    "**ä¸ºä»€ä¹ˆä¸ç”¨ `all_to_all_single`ï¼Ÿ**\n",
    "- `torch.distributed.all_to_all_single` çš„è¯­ä¹‰éå¸¸è´´åˆ MoEï¼šå®ƒå¤©ç„¶æ”¯æŒæ¯ä¸ª rank å‘/æ”¶ä¸ç­‰é•¿åˆ†ç‰‡ï¼ˆsplit sizesï¼‰ï¼Œå¯¹åº”â€œä¸åŒä¸“å®¶æ”¶åˆ°ä¸åŒ token æ•°é‡â€çš„ç°å®æƒ…å†µã€‚\n",
    "- ä½†`all_to_all_single`ä»…æ”¯æŒä¼ é€’dataï¼Œè€Œä¸ä¼ é€’gradï¼Œä¹Ÿå°±æ˜¯è¯´åœ¨è®­ç»ƒåœºæ™¯å¹¶ä¸é€‚ç”¨ã€‚ï¼ˆå¯ä»¥è‡ªå·±å†™ä¸ªè„šæœ¬checkä¸€ä¸‹ï¼‰\n",
    "- è¿™é‡Œé€‰æ‹©äº† **pad + `dist.nn.functional.all_gather`** çš„å®ç°ï¼šæŠŠæ¯ä¸ª rank çš„å‘é€æ¡ç›® pad åˆ°åŒæ ·é•¿åº¦ï¼Œå† gather åæŒ‰ meta è¿‡æ»¤ã€‚\n",
    "\n",
    "**æ ¸å¿ƒæ•°æ®ç»“æ„ï¼šbuffer + metadata**\n",
    "- bufferï¼šå­˜ token hiddenï¼ˆå½¢çŠ¶è¿‘ä¼¼ `[num_items, hidden_dim]`ï¼‰ã€‚\n",
    "- metadataï¼šä¸ºæ¯æ¡ token è®°å½•â€œè½¨è¿¹â€ï¼Œä»¥ä¾¿ combine æ—¶æŠŠç»“æœæ”¾å›æ­£ç¡®ä½ç½®ã€‚è¿™é‡Œç”¨ `META_DIM=5`ï¼š\n",
    "\n",
    "| å­—æ®µ | å«ä¹‰ | ä½œç”¨ |\n",
    "|---|---|---|\n",
    "| `global_exp` | å…¨å±€ä¸“å®¶ ID | å†³å®š token åº”è¯¥æ´¾å‘åˆ°å“ªä¸ª rank/å“ªä¸ªæœ¬åœ° expert |\n",
    "| `src_rank` | æº rank | combine æ—¶å†³å®šå›ä¼ åˆ°å“ªä¸ª rank |\n",
    "| `src_token` | æº token ä¸‹æ ‡ | combine æ—¶å›å¡«åˆ°å“ªä¸ª token ä½ç½® |\n",
    "| `src_k` | top-k åºå· | combine æ—¶å– `weights[token, k]` åšåŠ æƒ |\n",
    "| `pad` | padding æ ‡è®°/å ä½ | è¿™é‡Œç®€åŒ–ä¸º 0ï¼Œå ä½å³å¯ |\n",
    "\n",
    "**dispatch çš„è¾“å…¥/è¾“å‡ºï¼ˆæ¦‚å¿µä¸Šï¼‰**\n",
    "- è¾“å…¥ï¼š\n",
    "  - `dp_x`: å½“å‰ rank çš„ tokenï¼Œshape `[num_tokens, hidden_dim]`\n",
    "  - `indices`: æ¯ä¸ª token é€‰ä¸­çš„ä¸“å®¶ IDï¼Œshape `[num_tokens, topk]`\n",
    "- è¾“å‡ºï¼ˆè½æ¡¶åˆ°æœ¬åœ°ä¸“å®¶ï¼‰ï¼š\n",
    "  - `expert_num_tokens`: shape `[num_local_experts]`\n",
    "  - `expert_x`: shape `[num_local_experts, max_recv, hidden_dim]`ï¼ˆæŒ‰æœ¬åœ° expert åˆ†æ¡¶åçš„è¾“å…¥ï¼‰\n",
    "  - `expert_meta`: shape `[num_local_experts, max_recv, META_DIM]`\n",
    "\n",
    "**å®ç°ä¸Šçš„é‡è¦å–èˆï¼ˆä¹Ÿæ­£æ˜¯æ€§èƒ½ç“¶é¢ˆæ¥æºï¼‰**\n",
    "- éœ€è¦æŠŠå‘é€æ¡ç›® pad åˆ°ç»Ÿä¸€é•¿åº¦ï¼Œå¯¼è‡´é¢å¤–é€šä¿¡/æ‹·è´ï¼ˆå¯¹è´Ÿè½½ä¸å‡æ—¶å°¤å…¶æ˜æ˜¾ï¼‰ã€‚\n",
    "- é€šè¿‡ Python å¾ªç¯ç»„è£…/è½æ¡¶ï¼ˆè€Œä¸æ˜¯å‘é‡åŒ–/èåˆ kernelï¼‰ï¼Œå¼€é”€åå¤§ã€‚\n",
    "- è¿™æ˜¯ä¸ºäº†æŠŠâ€œæ•°æ®å¦‚ä½•æµåŠ¨â€è¯´æ¸…æ¥šï¼›åœ¨æ€»ç»“é‡Œä¼šåˆ—å‡ºæ›´è´´è¿‘çœŸå®è®­ç»ƒçš„ä¼˜åŒ–æ–¹å‘ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124a4031",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "moe_train"
    ]
   },
   "outputs": [],
   "source": [
    "# pytorch_all2all.py\n",
    "import torch.distributed as dist\n",
    "import torch.distributed.nn.functional as dist_nn\n",
    "\n",
    "# ---------------- All2All pytorch impl ----------------\n",
    "class PyTorchAllToAll:\n",
    "    META_DIM = 5  # global_exp, src_rank, src_token, src_k, pad\n",
    "\n",
    "    # åˆå§‹åŒ–ä¸€äº›åˆ†å¸ƒå¼éœ€è¦çš„å˜é‡\n",
    "    def __init__(self, cfg: MoEConfig, rank: int, world_size: int):\n",
    "        self.cfg = cfg\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "        # num experts per rank\n",
    "        self.num_local_experts = cfg.num_experts // world_size\n",
    "        # max recv tokens per rankï¼Œä¸æ»¡è¶³çš„ç”¨ padding\n",
    "        self.max_recv = cfg.max_num_tokens * world_size\n",
    "\n",
    "    # ---------- dispatch ----------\n",
    "    # dp_x å½“å‰ rankï¼ˆgpuï¼‰æ‹¥æœ‰çš„ tokenï¼Œshape = [num_tokens,hidden_dim]\n",
    "    # indices: æ¯ä¸ª token é€‰ä¸­çš„å…¨å±€ä¸“å®¶IDåˆ—è¡¨ï¼Œå½¢çŠ¶ [num_tokens, experts_per_token]ï¼Œå€¼åŸŸ [0, num_experts)ã€‚\n",
    "    # TODO experts_per_token å°±æ˜¯ topkï¼Œèƒ½ä¸èƒ½æ”¹æˆ topkï¼Ÿå¹¶ä¸”ä½œä¸ºä¸€ä¸ªå¯ä»¥ä»å¤–éƒ¨ä¼ å…¥çš„å‚æ•°ã€‚\n",
    "    def dispatch(self, dp_x: torch.Tensor, indices: torch.Tensor):\n",
    "        device = dp_x.device\n",
    "        cfg = self.cfg\n",
    "\n",
    "        # 1) æ„å»ºæ‰å¹³çš„å‘é€ buffer ä¸ meta\n",
    "        send_tokens = []\n",
    "        send_meta = []\n",
    "        for t, expert_list in enumerate(indices.tolist()):\n",
    "            for k, e in enumerate(expert_list):\n",
    "                send_tokens.append(dp_x[t].unsqueeze(0))\n",
    "                send_meta.append([e, self.rank, t, k, 0])\n",
    "        if send_tokens:\n",
    "            send_buf_flat = torch.cat(send_tokens, dim=0)\n",
    "            send_meta_flat = torch.tensor(send_meta, device=device, dtype=torch.int32)\n",
    "        else:\n",
    "            send_buf_flat = torch.empty((0, cfg.hidden_dim), device=device, dtype=cfg.in_dtype)\n",
    "            send_meta_flat = torch.empty((0, self.META_DIM), device=device, dtype=torch.int32)\n",
    "\n",
    "        # 2) äº¤æ¢å„ rank å‘é€æ¡æ•°ï¼Œpad åˆ°ç»Ÿä¸€é•¿åº¦\n",
    "        # è¿™æ ·åé¢å¯ä»¥ç”¨ all_gatherï¼ˆè¦æ±‚å„ rank tensor å½¢çŠ¶ä¸€è‡´ï¼‰æ¥å®ç°å¯åä¼ çš„é€šä¿¡ã€‚\n",
    "        send_items = torch.tensor([send_meta_flat.size(0)], device=device, dtype=torch.long)\n",
    "        all_items = [torch.zeros_like(send_items) for _ in range(self.world_size)]\n",
    "        dist.all_gather(all_items, send_items)\n",
    "        send_counts = [int(c.item()) for c in all_items]\n",
    "        max_items = max(send_counts)\n",
    "        pad_len = max_items - send_meta_flat.size(0)\n",
    "        if pad_len > 0:\n",
    "            pad_buf = torch.zeros(pad_len, cfg.hidden_dim, device=device, dtype=cfg.in_dtype)\n",
    "            pad_meta = torch.zeros(pad_len, self.META_DIM, device=device, dtype=torch.int32)\n",
    "            send_buf_flat = torch.cat([send_buf_flat, pad_buf], dim=0)\n",
    "            send_meta_flat = torch.cat([send_meta_flat, pad_meta], dim=0)\n",
    "\n",
    "        # 3) all_gather æ•°æ®ä¸ metaï¼ˆautograd-awareï¼‰\n",
    "        gathered_buf = dist_nn.all_gather(send_buf_flat)\n",
    "        gathered_meta = dist_nn.all_gather(send_meta_flat)\n",
    "        concat_buf = torch.cat(gathered_buf, dim=0)\n",
    "        concat_meta = torch.cat(gathered_meta, dim=0)\n",
    "\n",
    "        # 4) è¿‡æ»¤ç›®æ ‡ä¸ºæœ¬ rank çš„æ¡ç›®ï¼ˆæ ¹æ®å…¨å±€ä¸“å®¶ ID æ˜ å°„ rankï¼‰\n",
    "        global_eids = concat_meta[:, 0].to(torch.long)\n",
    "        dst_ranks = global_eids // self.num_local_experts\n",
    "        mask = dst_ranks == self.rank\n",
    "        valid_buf = concat_buf[mask]\n",
    "        valid_meta = concat_meta[mask]\n",
    "        total_recv = valid_buf.size(0)\n",
    "\n",
    "        # 5) è½æ¡¶åˆ°æœ¬åœ°ä¸“å®¶\n",
    "        expert_num_tokens = torch.zeros(self.num_local_experts, dtype=torch.int32, device=device)\n",
    "        expert_x = torch.empty(\n",
    "            (self.num_local_experts, self.max_recv, cfg.hidden_dim),\n",
    "            dtype=cfg.in_dtype,\n",
    "            device=device,\n",
    "        )\n",
    "        expert_meta = torch.empty(\n",
    "            (self.num_local_experts, self.max_recv, self.META_DIM),\n",
    "            dtype=torch.int32,\n",
    "            device=device,\n",
    "        )\n",
    "        for i in range(total_recv):\n",
    "            geid = int(valid_meta[i, 0].item())\n",
    "            local_eid = geid % self.num_local_experts\n",
    "            pos = expert_num_tokens[local_eid]\n",
    "            expert_x[local_eid, pos] = valid_buf[i]\n",
    "            expert_meta[local_eid, pos] = valid_meta[i]\n",
    "            expert_num_tokens[local_eid] += 1\n",
    "\n",
    "        return expert_num_tokens, expert_x, expert_meta\n",
    "\n",
    "    # ---------- combine ----------\n",
    "    def combine(\n",
    "        self,\n",
    "        out_tokens: torch.Tensor,  # output, (max num tokens, hidden_dim)\n",
    "        weights: torch.Tensor,  # topk weight\n",
    "        expert_meta: torch.Tensor,  # input\n",
    "        expert_y: torch.Tensor,  # input, (num_local_experts, max_num_tokens * num_dp, hidden_dim)\n",
    "        expert_num_tokens: torch.Tensor,\n",
    "    ):  # input\n",
    "        device = out_tokens.device\n",
    "        cfg = self.cfg\n",
    "\n",
    "        # å•æœºå•å¡ç›´æ¥èšåˆï¼Œé¿å…é€šä¿¡å†™å…¥ç ´åè®¡ç®—å›¾\n",
    "        if self.world_size == 1:\n",
    "            total_recv = int(expert_num_tokens.sum().item())\n",
    "            if total_recv == 0:\n",
    "                return out_tokens\n",
    "            idx = []\n",
    "            upd = []\n",
    "            for local_eid in range(self.num_local_experts):\n",
    "                cnt = int(expert_num_tokens[local_eid].item())\n",
    "                for j in range(cnt):\n",
    "                    meta = expert_meta[local_eid, j]\n",
    "                    src_token = int(meta[2].item())\n",
    "                    src_k = int(meta[3].item())\n",
    "                    w = weights[src_token, src_k].to(torch.float32)\n",
    "                    idx.append(src_token)\n",
    "                    upd.append(expert_y[local_eid, j].to(torch.float32) * w)\n",
    "            idx = torch.tensor(idx, device=device, dtype=torch.long)\n",
    "            updates = torch.stack(upd, dim=0)\n",
    "            out = torch.zeros_like(out_tokens, dtype=torch.float32)\n",
    "            out = out.index_add(0, idx, updates)\n",
    "            out = out + out_tokens.to(torch.float32)\n",
    "            return out.to(out_tokens.dtype)\n",
    "\n",
    "        # æ„å»ºæ‰å¹³çš„å‘é€ buffer ä¸ metaï¼ˆç›®æ ‡ rank = meta[:,1]ï¼‰\n",
    "        send_tokens = []\n",
    "        send_meta = []\n",
    "        for local_eid in range(self.num_local_experts):\n",
    "            cnt = int(expert_num_tokens[local_eid].item())\n",
    "            if cnt == 0:\n",
    "                continue\n",
    "            send_tokens.append(expert_y[local_eid, :cnt])\n",
    "            send_meta.append(expert_meta[local_eid, :cnt])\n",
    "        if send_tokens:\n",
    "            send_buf_flat = torch.cat(send_tokens, dim=0)\n",
    "            send_meta_flat = torch.cat(send_meta, dim=0)\n",
    "        else:\n",
    "            send_buf_flat = torch.empty((0, cfg.hidden_dim), device=device, dtype=cfg.out_dtype)\n",
    "            send_meta_flat = torch.empty((0, self.META_DIM), device=device, dtype=torch.int32)\n",
    "\n",
    "        # 1) äº¤æ¢æ¡æ•°ï¼Œpad åˆ°ç»Ÿä¸€é•¿åº¦\n",
    "        send_items = torch.tensor([send_meta_flat.size(0)], device=device, dtype=torch.long)\n",
    "        all_items = [torch.zeros_like(send_items) for _ in range(self.world_size)]\n",
    "        dist.all_gather(all_items, send_items)\n",
    "        send_counts = [int(c.item()) for c in all_items]\n",
    "        max_items = max(send_counts)\n",
    "        pad_len = max_items - send_meta_flat.size(0)\n",
    "        if pad_len > 0:\n",
    "            pad_buf = torch.zeros(pad_len, cfg.hidden_dim, device=device, dtype=cfg.out_dtype)\n",
    "            pad_meta = torch.zeros(pad_len, self.META_DIM, device=device, dtype=torch.int32)\n",
    "            send_buf_flat = torch.cat([send_buf_flat, pad_buf], dim=0)\n",
    "            send_meta_flat = torch.cat([send_meta_flat, pad_meta], dim=0)\n",
    "\n",
    "        # 2) all_gather æ•°æ®å’Œå…ƒä¿¡æ¯\n",
    "        gathered_buf = dist_nn.all_gather(send_buf_flat)\n",
    "        gathered_meta = dist_nn.all_gather(send_meta_flat)\n",
    "        concat_buf = torch.cat(gathered_buf, dim=0)\n",
    "        concat_meta = torch.cat(gathered_meta, dim=0)\n",
    "\n",
    "        # 3) è¿‡æ»¤ç›®æ ‡ä¸ºæœ¬ rank çš„æ¡ç›®ï¼ˆmeta[1] æ˜¯ src_rankï¼Œä½œä¸ºå›ä¼ ç›®çš„åœ°ï¼‰\n",
    "        dst_mask = concat_meta[:, 1].to(torch.long) == self.rank\n",
    "        if not torch.any(dst_mask):\n",
    "            return out_tokens\n",
    "        recv_buf = concat_buf[dst_mask]\n",
    "        recv_meta = concat_meta[dst_mask]\n",
    "\n",
    "        # 4) èšåˆå›æº token\n",
    "        idx = recv_meta[:, 2].to(torch.long)      # src_token\n",
    "        src_k = recv_meta[:, 3].to(torch.long)    # topk åºå·\n",
    "        weight = weights[idx, src_k].to(torch.float32)\n",
    "        updates = recv_buf.to(torch.float32) * weight.unsqueeze(1)\n",
    "\n",
    "        out = torch.zeros_like(out_tokens, dtype=torch.float32)\n",
    "        out = out.index_add(0, idx, updates)\n",
    "        out = out + out_tokens.to(torch.float32)\n",
    "        return out.to(out_tokens.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d68eb75",
   "metadata": {},
   "source": [
    "### class Expert and class EPMoE\n",
    "\n",
    "è¿™é‡Œå®šä¹‰ï¼š\n",
    "- `Expert`ï¼šå•ä¸ªä¸“å®¶ï¼ˆç”¨ä¸¤å±‚ MLP ä»£è¡¨ï¼‰ã€‚\n",
    "- `EPMoE`ï¼šEP å¹¶è¡Œçš„ MoE layerï¼ŒåŒ…å« gateã€ä¸“å®¶åˆ†ç‰‡ã€dispatch/combineã€‚\n",
    "\n",
    "ä¸ºäº†é˜…è¯»æ–¹ä¾¿ï¼Œå¯ä»¥æŠŠä¸€æ¬¡å‰å‘ç†è§£æˆ 5 æ­¥ï¼š\n",
    "\n",
    "1. **gate + top-k**ï¼š`gate(x)` å¾—åˆ°æ¯ä¸ª token å¯¹æ¯ä¸ªä¸“å®¶çš„æ‰“åˆ†ï¼Œsoftmax åå– top-kï¼Œå¾—åˆ°ï¼š\n",
    "   - `indices`: `[num_tokens, topk]`ï¼ˆæ¯ä¸ª token é€‰ä¸­çš„å…¨å±€ä¸“å®¶ IDï¼‰\n",
    "   - `weights`: `[num_tokens, topk]`ï¼ˆå¯¹åº”çš„æ¦‚ç‡/æƒé‡ï¼‰\n",
    "2. **aux lossï¼ˆè´Ÿè½½å‡è¡¡ï¼‰**ï¼šè®­ç»ƒæ—¶é¢å¤–è¿”å›ä¸€ä¸ªè¾…åŠ©æŸå¤±ï¼Œé¼“åŠ±è·¯ç”±æ›´å‡åŒ€ï¼Œé¿å…å°‘æ•°ä¸“å®¶è¿‡è½½ã€‚\n",
    "3. **dispatch**ï¼šæŒ‰ `indices` æŠŠ token å‘åˆ°â€œä¸“å®¶æ‰€åœ¨ rankâ€ã€‚\n",
    "4. **local expert forward**ï¼šæ¯ä¸ª rank åªå¯¹è‡ªå·±çš„æœ¬åœ°ä¸“å®¶åšå‰å‘ï¼ˆåªç®—æœ¬åœ°å‚æ•°ï¼‰ã€‚\n",
    "5. **combine**ï¼šæŠŠä¸“å®¶è¾“å‡ºæŒ‰ `expert_meta` å›ä¼ åˆ°æº rankï¼Œå¹¶æŒ‰ `weights` èšåˆå› token åºåˆ—ã€‚\n",
    "\n",
    "å®ç°ç»†èŠ‚/æ³¨æ„ï¼š\n",
    "- `weights` ç”¨ `float32` åšåŠ æƒï¼Œé¿å…åŠç²¾åº¦ä¸‹ç´¯åŠ è¯¯å·®æ”¾å¤§ã€‚\n",
    "- å•å¡è·¯å¾„ï¼ˆ`world_size==1`ï¼‰ä¸“é—¨åšäº†ä¸€æ¡â€œæ— é€šä¿¡â€çš„åˆ†æ”¯ï¼šæ—¢é¿å…é€šä¿¡å¼€é”€ï¼Œä¹Ÿå°½é‡ä¿ä½ autograd å›¾ã€‚\n",
    "- å¤šå¡è·¯å¾„ç”¨ `PyTorchAllToAll` ä½œä¸ºé€šä¿¡åç«¯ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c5a5930",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "moe_train"
    ]
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "\n",
    "# ä¸“å®¶æ¨¡å—\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.GELU(),  \n",
    "            nn.Linear(hidden_dim, output_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)  \n",
    "\n",
    "class EPMoE(nn.Module):\n",
    "    \"\"\"\n",
    "    Expert-Parallel MoE layer.\n",
    "\n",
    "    - gate: replicated across ranks (wrap with DDP outside if world_size > 1)\n",
    "    - experts: sharded across ranks (each rank owns num_experts/world_size experts)\n",
    "    - comm: dispatch/combine via PyTorchAllToAll\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: MoEConfig, rank: int | None = None, world_size: int | None = None):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.rank = dist.get_rank() if rank is None else rank\n",
    "        self.world_size = dist.get_world_size() if world_size is None else world_size\n",
    "\n",
    "        if cfg.num_experts % self.world_size != 0:\n",
    "            raise ValueError(\"num_experts must be divisible by world_size\")\n",
    "\n",
    "        # ATAï¼ˆall-to-allï¼‰é€šä¿¡ç®—å­ï¼šè´Ÿè´£ EP çš„ token æ´¾å‘ï¼ˆdispatchï¼‰ä¸å›æ”¶ï¼ˆcombineï¼‰\n",
    "        self.ata = PyTorchAllToAll(cfg, rank=self.rank, world_size=self.world_size)\n",
    "        # gate åœ¨æ¯ä¸ª rank éƒ½æœ‰ä¸€ä»½ï¼›è®­ç»ƒæ—¶å»ºè®®åœ¨å¤–éƒ¨ç”¨ DDP åŒ…èµ·æ¥åšå‚æ•°/æ¢¯åº¦åŒæ­¥\n",
    "        self.gate = nn.Linear(cfg.hidden_dim, cfg.num_experts)\n",
    "\n",
    "        # experts æŒ‰ rank åˆ†ç‰‡ï¼šæ¯ä¸ª rank åªæ‹¥æœ‰ num_local_experts ä¸ªä¸“å®¶å‚æ•°\n",
    "        self.num_local_experts = cfg.num_experts // self.world_size\n",
    "        self.experts = nn.ModuleList(\n",
    "            [Expert(cfg.hidden_dim, cfg.hidden_dim * 4, cfg.hidden_dim) for _ in range(self.num_local_experts)]\n",
    "        )\n",
    "\n",
    "    def _aux_loss(self, probs: torch.Tensor, indices: torch.Tensor) -> torch.Tensor:\n",
    "        cfg = self.cfg\n",
    "        # è´Ÿè½½å‡è¡¡è¾…åŠ©æŸå¤±ï¼ˆå’Œ my_moe/layers/moe_layer.py ä¿æŒä¸€è‡´ï¼‰\n",
    "        # ç›®çš„ï¼šé¿å… gate æŠŠ token è¿‡åº¦è·¯ç”±åˆ°å°‘æ•°ä¸“å®¶ï¼Œå¯¼è‡´ä¸“å®¶â€œå¿™é—²ä¸å‡â€\n",
    "        importance = probs.sum(0)  # [num_experts]\n",
    "        importance_loss = torch.var(importance) / (cfg.num_experts**2)\n",
    "        mask = torch.zeros_like(probs, dtype=torch.bool)\n",
    "        mask.scatter_(1, indices, True)\n",
    "        routing_probs = probs * mask\n",
    "        expert_usage = mask.float().mean(0)\n",
    "        routing_weights = routing_probs.mean(0)\n",
    "        load_balance_loss = cfg.num_experts * (expert_usage * routing_weights).sum()\n",
    "        return importance_loss + load_balance_loss\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        cfg = self.cfg\n",
    "        # 1) routingï¼šè®¡ç®—æ¯ä¸ª token å¯¹æ¯ä¸ªä¸“å®¶çš„åŒ¹é…åˆ†æ•°ï¼Œç„¶åå– top-k ä¸“å®¶\n",
    "        logits = self.gate(x)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        weights, indices = torch.topk(probs, cfg.experts_per_token, dim=-1)\n",
    "        # indices: ä¸“å®¶ IDï¼›weights: å¯¹åº”æ¦‚ç‡ï¼ˆåç»­ combine ä¼šæŒ‰ weights åŠ æƒèšåˆï¼‰\n",
    "        indices = indices.to(torch.int64)\n",
    "        weights = weights.to(torch.float32)\n",
    "\n",
    "        # 2) aux lossï¼šè®­ç»ƒæ—¶è¿”å›ï¼Œæ¨ç†æ—¶ä¸º 0\n",
    "        aux_loss = self._aux_loss(probs, indices) if self.training else torch.tensor(0.0, device=x.device)\n",
    "\n",
    "        # 3) dispatchï¼šæŠŠ token æ ¹æ®â€œä¸“å®¶æ‰€åœ¨ rankâ€æ´¾å‘å‡ºå»\n",
    "        #    - å•å¡ï¼šä¸èµ°é€šä¿¡ï¼Œç›´æ¥åœ¨æœ¬åœ°æŒ‰ä¸“å®¶åˆ†æ¡¶ï¼ˆä¿ç•™å¯¹ x çš„è®¡ç®—å›¾ï¼‰\n",
    "        #    - å¤šå¡ï¼šèµ° ATA dispatchï¼ˆall_to_all / all_gather ç­‰å®ç°ç»†èŠ‚åœ¨ reference.pyï¼‰\n",
    "        if self.world_size == 1:\n",
    "            token_map = [[] for _ in range(self.ata.num_local_experts)]\n",
    "            for t, expert_list in enumerate(indices.tolist()):\n",
    "                for k, e in enumerate(expert_list):\n",
    "                    local_eid = e % self.ata.num_local_experts\n",
    "                    token_map[local_eid].append((t, k, e))\n",
    "\n",
    "            expert_num = torch.tensor([len(lst) for lst in token_map], device=x.device, dtype=torch.int32)\n",
    "            # expert_meta: æ¯æ¡æ´¾å‘ token çš„å…ƒä¿¡æ¯ï¼ˆç”¨äº combine å›å¡«ï¼‰\n",
    "            # META_DIM=5: [global_exp, src_rank, src_token, src_k, pad]\n",
    "            expert_meta = torch.zeros(\n",
    "                (self.ata.num_local_experts, self.ata.max_recv, self.ata.META_DIM),\n",
    "                device=x.device,\n",
    "                dtype=torch.int32,\n",
    "            )\n",
    "            expert_inputs = []\n",
    "            for local_eid, lst in enumerate(token_map):\n",
    "                for pos, (t, k, e) in enumerate(lst):\n",
    "                    expert_meta[local_eid, pos, 0] = e\n",
    "                    expert_meta[local_eid, pos, 1] = self.rank\n",
    "                    expert_meta[local_eid, pos, 2] = t\n",
    "                    expert_meta[local_eid, pos, 3] = k\n",
    "                idx = [t for t, _, _ in lst]\n",
    "                expert_inputs.append(x[idx] if idx else None)\n",
    "            expert_x = None\n",
    "        else:\n",
    "            expert_num, expert_x, expert_meta = self.ata.dispatch(x, indices)\n",
    "            expert_inputs = None\n",
    "\n",
    "        # 4) local expert forwardï¼šæ¯ä¸ª rank åªè®¡ç®—è‡ªå·±æŒæœ‰çš„æœ¬åœ°ä¸“å®¶\n",
    "        expert_y = torch.zeros(\n",
    "            (self.ata.num_local_experts, self.ata.max_recv, cfg.hidden_dim),\n",
    "            device=x.device,\n",
    "            dtype=cfg.out_dtype,\n",
    "        )\n",
    "        # TODOï¼šè¿™ä¸ªåœ°æ–¹å¯ä»¥ç”¨bmmä»£æ›¿å¾ªç¯ï¼Œè·å¾—å¤§å¹…ä¼˜åŒ–\n",
    "        for local_eid in range(self.ata.num_local_experts):\n",
    "            cnt = int(expert_num[local_eid].item())\n",
    "            if cnt == 0:\n",
    "                continue\n",
    "            if self.world_size == 1:\n",
    "                x_slice = expert_inputs[local_eid].to(torch.float32)\n",
    "            else:\n",
    "                x_slice = expert_x[local_eid, :cnt].to(torch.float32)\n",
    "            y_slice = self.experts[local_eid](x_slice).to(cfg.out_dtype)\n",
    "            expert_y[local_eid, :cnt] = y_slice\n",
    "\n",
    "        # 5) combineï¼šæŠŠä¸“å®¶è¾“å‡ºæŒ‰ meta å›ä¼ åˆ°åŸ rankï¼Œå¹¶æŒ‰ top-k æƒé‡åŠ æƒèšåˆå› token åºåˆ—\n",
    "        out_tokens = torch.zeros(cfg.max_num_tokens, cfg.hidden_dim, device=x.device, dtype=cfg.out_dtype)\n",
    "        out_tokens = self.ata.combine(out_tokens, weights, expert_meta, expert_y, expert_num)\n",
    "        out_tokens = out_tokens[: x.shape[0]]\n",
    "        return out_tokens, aux_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144ef115",
   "metadata": {},
   "source": [
    "## è®­ç»ƒ\n",
    "\n",
    "è¿™ä¸€èŠ‚ç”¨ä¸€ä¸ª toy ä»»åŠ¡æŠŠè®­ç»ƒé—­ç¯è·‘é€šï¼Œç›®æ ‡ä¸æ˜¯â€œé«˜æ•ˆè®­ç»ƒå‡ºå¥½æ¨¡å‹â€ï¼Œè€Œæ˜¯éªŒè¯ï¼š\n",
    "\n",
    "- gate çš„è·¯ç”±ï¼ˆtop-kï¼‰èƒ½å·¥ä½œï¼›\n",
    "- ataç®—å­åˆ†å¸ƒå¼é€šä¿¡å·¥ä½œæ­£å¸¸ï¼š\n",
    "    - token èƒ½æ­£ç¡® dispatch åˆ°ä¸“å®¶æ‰€åœ¨ rankï¼Œä¸“å®¶ç®—å®Œèƒ½æ­£ç¡® combine å›æ¥ï¼›\n",
    "    - åå‘ä¼ æ’­èƒ½ç©¿è¿‡ gate / experts / é€šä¿¡è·¯å¾„ï¼›\n",
    "- Lossæ­£å¸¸ä¸‹é™ï¼›æ—¥å¿—ä¸ tensorboard æŒ‡æ ‡èƒ½å¸®åŠ©æˆ‘ä»¬è§‚å¯Ÿ loss ä¸è€—æ—¶ã€‚\n",
    "\n",
    "è®­ç»ƒè®¾ç½®ï¼š\n",
    "- æ¨¡æ‹Ÿä»»åŠ¡ï¼šç”¨ `target_proj`ï¼ˆä¸€ä¸ªå›ºå®šçš„çº¿æ€§å±‚ï¼‰ç”Ÿæˆç›‘ç£ä¿¡å· `y`ï¼Œè®© MoE å»æ‹Ÿåˆè¿™ä¸ªæ˜ å°„ã€‚\n",
    "- æ€»æŸå¤±ï¼š`task_loss + aux_alpha * aux_loss`ã€‚\n",
    "- rank0 æ¯éš” `log_interval` æ‰“å°ä¸€æ¬¡ loss å’ŒåŒºé—´è€—æ—¶ï¼›åŒæ—¶å†™å…¥ tensorboardï¼ˆ`TB_LOGDIR`ï¼‰ã€‚\n",
    "\n",
    "ä½ åº”è¯¥çœ‹åˆ°çš„ç°è±¡ï¼š\n",
    "- `loss/task` ä¼šé€æ¸ä¸‹é™ï¼ˆtoy ä»»åŠ¡é€šå¸¸å¾ˆå¿«èƒ½ä¸‹é™ï¼‰ï¼›\n",
    "- `loss/aux` çš„é‡çº§ä¸ `aux_alpha`/è·¯ç”±åˆ†å¸ƒæœ‰å…³ï¼Œä¸ä¸€å®šå•è°ƒï¼›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad1794a1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "moe_train"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "\n",
    "def init_distributed():\n",
    "    \"\"\"Init process group if not already done.\"\"\"\n",
    "    if dist.is_initialized():\n",
    "        return\n",
    "    backend = \"nccl\" if torch.cuda.is_available() else \"gloo\"\n",
    "    dist.init_process_group(backend=backend)\n",
    "\n",
    "\n",
    "def train_tiny_ep(\n",
    "    cfg: MoEConfig,\n",
    "    steps: int = 10,\n",
    "    bsz: int = 16,\n",
    "    lr: float = 5e-4,\n",
    "    log_interval: int = 1000,\n",
    "    aux_alpha: float = 1e-2,\n",
    "    profile: bool = False,\n",
    "):\n",
    "    \"\"\"Minimal EP-only training loop using EPMoE layer.\"\"\"\n",
    "    # åˆ†å¸ƒå¼ç¯å¢ƒåˆå§‹åŒ–\n",
    "    rank = dist.get_rank()\n",
    "    world_size = dist.get_world_size()\n",
    "    # è®¾å¤‡åˆ†é…ï¼šcuda: 0, 1, ...\n",
    "    device = (\n",
    "        torch.device(f\"cuda:{int(os.environ.get('LOCAL_RANK', 0))}\")\n",
    "        if torch.cuda.is_available()\n",
    "        else torch.device(\"cpu\")\n",
    "    )\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.set_device(device.index)     # ç»‘å®šrankåˆ°å¯¹åº”çš„GPUè®¾å¤‡\n",
    "\n",
    "    # ä½¿ç”¨EPMoEå±‚æ„å»ºæ¨¡å‹ï¼ˆåŒ…å«gateã€expertså’Œall-to-allé€šä¿¡ï¼‰\n",
    "    model = EPMoE(cfg, rank=rank, world_size=world_size).to(device)\n",
    "    if world_size > 1:\n",
    "        model.gate = DDP(model.gate, device_ids=[device] if device.type == \"cuda\" else None)\n",
    "\n",
    "    # è®­ç»ƒé…ç½®ï¼šä¼˜åŒ–å™¨ã€ç›®æ ‡å‡½æ•°å’Œç›‘æ§å·¥å…·\n",
    "    opt = torch.optim.AdamW(list(model.gate.parameters()) + list(model.experts.parameters()), lr=lr)\n",
    "    target_proj = torch.nn.Linear(cfg.hidden_dim, cfg.hidden_dim, bias=False).to(device)  # æ¨¡æ‹Ÿä»»åŠ¡ï¼šè®­ç»ƒMoEç½‘ç»œæ‹Ÿåˆçº¿æ€§å˜æ¢\n",
    "    target_proj.requires_grad_(False)                                                       # ç›®æ ‡ç½‘ç»œä¸å‚ä¸è®­ç»ƒï¼Œä»…ç”¨äºç”Ÿæˆæ ‡ç­¾\n",
    "    mse = torch.nn.MSELoss()\n",
    "    writer = SummaryWriter(log_dir=os.environ.get(\"TB_LOGDIR\")) if rank == 0 else None\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    last_log_time = start_time\n",
    "    for step in range(steps):\n",
    "        # 1) ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®\n",
    "        x = torch.randn(bsz, cfg.hidden_dim, device=device, dtype=cfg.in_dtype)\n",
    "        with torch.no_grad():\n",
    "            y = target_proj(x.float()).to(cfg.out_dtype)\n",
    "\n",
    "        # 2) EPMoEå‰å‘è®¡ç®—ï¼ˆåŒ…å«è·¯ç”±ã€all-to-allé€šä¿¡ã€ä¸“å®¶è®¡ç®—å’Œç»“æœèšåˆï¼‰\n",
    "        out_tokens, aux_loss = model(x)\n",
    "\n",
    "        # 3) æŸå¤±è®¡ç®—å’Œåå‘ä¼ æ’­\n",
    "        task_loss = mse(out_tokens.float(), y)\n",
    "        total_loss = task_loss + aux_alpha * aux_loss\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        total_loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        interval_s = None\n",
    "        if rank == 0 and step % log_interval == 0:\n",
    "            now = time.perf_counter()\n",
    "            interval_s = 0.0 if step == 0 else (now - last_log_time)\n",
    "            last_log_time = now\n",
    "            print(\n",
    "                f\"[step {step:05d}] task={task_loss.item():.4f} aux={aux_loss.item():.4f} \"\n",
    "                f\"total={total_loss.item():.4f} interval_s={interval_s:.2f}\"\n",
    "            )\n",
    "        if writer:\n",
    "            writer.add_scalar(\"loss/task\", task_loss.item(), step)\n",
    "            writer.add_scalar(\"loss/aux\", aux_loss.item(), step)\n",
    "            writer.add_scalar(\"loss/total\", total_loss.item(), step)\n",
    "            if interval_s is not None:\n",
    "                writer.add_scalar(\"time/interval_s\", interval_s, step)\n",
    "    \n",
    "    if profile and rank == 0:\n",
    "        elapsed = time.perf_counter() - start_time\n",
    "        print(f\"[ep] total {steps} steps time: {elapsed:.2f}s | {elapsed/steps*1000:.2f} ms/step\")\n",
    "\n",
    "    dist.barrier()\n",
    "    if writer:\n",
    "        writer.flush()\n",
    "        writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eb636d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**é—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ**\n",
    "\n",
    "åœ¨ notebook é‡Œå¤šå¡è·‘è®­ç»ƒä¸æ–¹ä¾¿ï¼šå†…æ ¸é»˜è®¤ world-size=1ï¼Œä¸” torchrun å¯åŠ¨çš„æ–°è¿›ç¨‹æ— æ³•å…±äº« notebook å·²åŠ è½½çš„ä»£ç /å†…å­˜ã€‚\n",
    "\n",
    "**æ€ä¹ˆè§£å†³çš„ï¼Ÿ**\n",
    "\n",
    "* æˆ‘ä»¬ä» notebook æŠ½å–å¸¦ tag çš„å…³é”® code cellsï¼ˆå¦‚éƒ½æ‰“ moe_coreï¼Œæˆ–ç»†åˆ† tagsï¼‰ï¼Œæ‹¼æˆä¸´æ—¶ launcher.py\n",
    "* ç„¶åç”¨ subprocess è°ƒ torchrun launcher.pyï¼Œè®©å®ƒè‡ªåŠ¨ spawn å¤šè¿›ç¨‹ã€‚\n",
    "* ç”¨ tag æŠ½å–è€Œä¸æ˜¯å›ºå®šä¸‹æ ‡ï¼Œè¿™æ ·ä½ å¯ä»¥éšæ„æ’å…¥/è°ƒæ•´ markdown æˆ–å…¶ä»– cellsï¼Œä¸ä¼šå½±å“ launcher æŠ½å–ã€‚\n",
    "\n",
    "è¿è¡Œæµç¨‹ï¼šåœ¨ notebook ä¸­è°ƒç”¨ torchrun --nproc_per_node=<N> launcher.pyï¼Œlauncher.py å†…éƒ¨ä¼š init_process_groupã€åˆ›å»ºæ¨¡å‹/è®­ç»ƒå¾ªç¯ï¼Œå®Œæˆåˆ†å¸ƒå¼è®­ç»ƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b386d1af",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /root/zomi_moe/06AlgoData/02MoE/ep_launcher/launcher.py\n",
      "Wrote launcher to /root/zomi_moe/06AlgoData/02MoE/ep_launcher/launcher.py\n",
      "Running:\n",
      " torchrun --nproc_per_node=2 /root/zomi_moe/06AlgoData/02MoE/ep_launcher/launcher.py\n",
      "[step 00000] task=0.3228 aux=0.7188 total=0.3300 interval_s=0.00\n",
      "[step 01000] task=0.2033 aux=1.0053 total=0.2133 interval_s=11.74\n",
      "[step 02000] task=0.1880 aux=0.8309 total=0.1963 interval_s=11.22\n",
      "[step 03000] task=0.1810 aux=0.5976 total=0.1870 interval_s=11.20\n",
      "[step 04000] task=0.1781 aux=0.5668 total=0.1838 interval_s=11.17\n",
      "[step 05000] task=0.1801 aux=0.5431 total=0.1855 interval_s=11.02\n",
      "[step 06000] task=0.1687 aux=0.5110 total=0.1738 interval_s=11.35\n",
      "[step 07000] task=0.1789 aux=0.4121 total=0.1830 interval_s=11.16\n",
      "[step 08000] task=0.1818 aux=0.4152 total=0.1859 interval_s=11.17\n",
      "[step 09000] task=0.1774 aux=0.3601 total=0.1810 interval_s=11.28\n",
      "EP MoE tiny training finished.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['torchrun', '--nproc_per_node=2', '/root/zomi_moe/06AlgoData/02MoE/ep_launcher/launcher.py'], returncode=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# è¯´æ˜ï¼štorchrun ä¼š spawn æ–°è¿›ç¨‹æ‰§è¡Œä¸€ä¸ªâ€œè„šæœ¬æ–‡ä»¶â€ï¼Œä¸ä¼šå…±äº« notebook å†…å­˜ã€‚\n",
    "# è¿™é‡Œç”¨æœ€ç®€å•çš„æ–¹å¼ï¼šè¯»å‡ºéœ€è¦çš„ code cell æ‹¼æˆä¸€ä¸ªè„šæœ¬ï¼Œç”¨ %%writefile å†™åˆ°å½“å‰å·¥ä½œç›®å½•ä¸‹çš„ ep_launcher/launcher.pyï¼Œå† torchrun è·‘å®ƒã€‚\n",
    "\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from IPython import get_ipython\n",
    "\n",
    "# notebook è·¯å¾„ï¼ˆæŒ‰å½“å‰ repo ç›¸å¯¹è·¯å¾„è¯»å–ï¼‰\n",
    "NOTEBOOK_PATH = Path(\"CODE03DistMoE.ipynb\")\n",
    "TAG_LIST = [\"moe_train\"]\n",
    "\n",
    "def _cells_by_tags(nb_path: Path, tags: list[str]) -> list[int]:\n",
    "    nb = json.loads(nb_path.read_text(encoding=\"utf-8\"))\n",
    "    hit = []\n",
    "    for i, c in enumerate(nb[\"cells\"]):\n",
    "        if c.get(\"cell_type\") != \"code\":\n",
    "            continue\n",
    "        t = c.get(\"metadata\", {}).get(\"tags\", [])\n",
    "        if any(tag in t for tag in tags):\n",
    "            hit.append(i)\n",
    "    return hit\n",
    "\n",
    "cell_indices = _cells_by_tags(NOTEBOOK_PATH, TAG_LIST)\n",
    "# print(cell_indices)\n",
    "\n",
    "# åœ¨å½“å‰å·¥ä½œç›®å½•ä¸‹ä½¿ç”¨å›ºå®šç›®å½•ï¼Œä¾¿äºä¿ç•™è„šæœ¬å’Œ tensorboard æ—¥å¿—\n",
    "LAUNCH_DIR = Path.cwd() / \"ep_launcher\"\n",
    "LAUNCH_DIR.mkdir(exist_ok=True)\n",
    "SCRIPT_PATH = LAUNCH_DIR / \"launcher.py\"\n",
    "LOG_DIR = LAUNCH_DIR / \"runs\"\n",
    "LOG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "moe_cfg = dict(\n",
    "    num_experts=16,\n",
    "    experts_per_token=2,\n",
    "    hidden_dim=256,\n",
    "    max_num_tokens=128,\n",
    ")\n",
    "\n",
    "train_cfg = dict(\n",
    "    steps=10000,\n",
    "    bsz=32,\n",
    "    lr=5e-4,\n",
    "    aux_alpha=1e-2,\n",
    "    log_interval=1000,\n",
    "    profile=False,\n",
    ")\n",
    "\n",
    "dist_cfg = dict(\n",
    "    nproc_per_node=2,  # æ”¹æˆä½ çš„ GPU æ•°\n",
    ")\n",
    "\n",
    "\n",
    "def _read_cells_as_py(nb_path: Path, indices: list[int]) -> str:\n",
    "    nb = json.loads(nb_path.read_text(encoding=\"utf-8\"))\n",
    "    parts = []\n",
    "    for i in indices:\n",
    "        parts.append(\"\".join(nb[\"cells\"][i][\"source\"]))\n",
    "        parts.append(\"\\n\\n\")\n",
    "    return \"\".join(parts)\n",
    "\n",
    "\n",
    "if not NOTEBOOK_PATH.exists():\n",
    "    raise FileNotFoundError(f\"æ‰¾ä¸åˆ° notebook: {NOTEBOOK_PATH}\")\n",
    "\n",
    "core = _read_cells_as_py(NOTEBOOK_PATH, cell_indices)\n",
    "\n",
    "runner = f'''\\\\\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "MOE_CFG = {moe_cfg!r}\n",
    "TRAIN_CFG = {train_cfg!r}\n",
    "\n",
    "def main():\n",
    "    init_distributed()\n",
    "    cfg = MoEConfig(**MOE_CFG, in_dtype=torch.float32, out_dtype=torch.float32)\n",
    "    try:\n",
    "        train_tiny_ep(cfg, **TRAIN_CFG)\n",
    "        if dist.get_rank() == 0:\n",
    "            print(\"EP MoE tiny training finished.\")\n",
    "    finally:\n",
    "        dist.destroy_process_group()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "ip = get_ipython()\n",
    "if ip is None:\n",
    "    raise RuntimeError(\"éœ€è¦åœ¨ Jupyter/IPython ç¯å¢ƒä¸‹è¿è¡Œï¼ˆä¾èµ– %%writefile magicï¼‰ã€‚\")\n",
    "\n",
    "ip.run_cell_magic(\"writefile\", str(SCRIPT_PATH), core + \"\\n\\n\" + runner)\n",
    "print(f\"Wrote launcher to {SCRIPT_PATH}\")\n",
    "\n",
    "cmd = [\n",
    "    \"torchrun\",\n",
    "    \"--nproc_per_node=\" + str(dist_cfg[\"nproc_per_node\"]),\n",
    "    str(SCRIPT_PATH),\n",
    "]\n",
    "\n",
    "env = os.environ.copy()\n",
    "env.setdefault(\"TB_LOGDIR\", str(LOG_DIR))\n",
    "\n",
    "print(\"Running:\\n\", \" \".join(cmd))\n",
    "subprocess.run(cmd, check=True, env=env, cwd=str(LAUNCH_DIR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d29567",
   "metadata": {},
   "source": [
    "# æ€»ç»“"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6dc276",
   "metadata": {},
   "source": [
    "## æˆ‘ä»¬å®ç°äº†ä»€ä¹ˆï¼Ÿ\n",
    "\n",
    "è¿™ä¸€ä»½å®ç°åˆ»æ„è¿½æ±‚â€œæœ€å°é—­ç¯ã€å¯è¯»å¯è·‘â€ï¼Œæ ¸å¿ƒè´¡çŒ®æ˜¯æŠŠ EP-MoE çš„æ•°æ®æµåœ¨ä»£ç é‡Œèµ°é€šï¼š\n",
    "\n",
    "- **EP çš„æœ€å°å¯è¿è¡Œç‰ˆæœ¬**ï¼š\n",
    "  - gateï¼ˆå¤åˆ¶åœ¨æ¯ä¸ª rankï¼‰è´Ÿè´£å¯¹ token åšè·¯ç”±æ‰“åˆ†ï¼›\n",
    "  - expertsï¼ˆæŒ‰ rank åˆ†ç‰‡ï¼‰åªåœ¨æœ¬åœ°è®¡ç®—ï¼Œé¿å…æ¯å¼ å¡éƒ½æŒæœ‰å…¨é‡ä¸“å®¶ï¼›\n",
    "  - dispatch/combine è´Ÿè´£è·¨ rank æ´¾å‘ token ä¸å›ä¼ ç»“æœã€‚\n",
    "- **è‡ªå®šä¹‰çš„æ¨¡æ‹Ÿä»»åŠ¡**ï¼š\n",
    "  - ç”¨ä¸€ä¸ªå›ºå®šçš„ `target_proj` ç”Ÿæˆç›‘ç£ä¿¡å·ï¼Œè®© MoE æ‹Ÿåˆçº¿æ€§æ˜ å°„ï¼›\n",
    "  - è®­ç»ƒæŸå¤±ç”± `task_loss + aux_alpha * aux_loss` æ„æˆï¼š`task_loss` é©±åŠ¨ä»»åŠ¡æ‹Ÿåˆï¼Œ`aux_loss` çº¦æŸè·¯ç”±æ›´å‡è¡¡ï¼›\n",
    "  - é€šè¿‡ `torchrun` å¯åŠ¨å¤šè¿›ç¨‹ï¼ŒéªŒè¯å¤šå¡ç¯å¢ƒä¸‹ forward/backward/optimizer step éƒ½èƒ½è·‘é€šã€‚\n",
    "- **å¯è§£é‡Šçš„é€šä¿¡æ•°æ®ç»“æ„**ï¼š\n",
    "  - å¯¹æ¯æ¡æ´¾å‘å‡ºå»çš„ token éƒ½è®°å½• `expert_meta`ï¼ˆå…¨å±€ä¸“å®¶ã€æº rankã€æº token ä¸‹æ ‡ã€topk åºå·ç­‰ï¼‰ï¼Œcombine æ—¶èƒ½ç²¾ç¡®å›å¡«ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d702d65",
   "metadata": {},
   "source": [
    "## å½“å‰å®ç°çš„ä¸»è¦æ€§èƒ½ç“¶é¢ˆ\n",
    "\n",
    "è¿™ä»½å®ç°æ˜¯â€œæ•™å­¦ç‰ˆâ€ï¼Œä¸ºäº†å¯è¯»æ€§ç‰ºç‰²äº†ä¸å°‘æ€§èƒ½ï¼Œä¸»è¦ç“¶é¢ˆé›†ä¸­åœ¨ä¸‰ç±»ï¼š\n",
    "\n",
    "1. **Python å¾ªç¯ä¸æ•°æ®æ¬è¿ï¼ˆCPU ä¾§å¼€é”€ + kernel launch é¢‘ç¹ï¼‰**\n",
    "   - dispatch æ—¶æŠŠ token å±•å¹³ã€é€æ¡ append/concatï¼›\n",
    "   - è½æ¡¶æ—¶é€æ¡å†™å…¥ `expert_x/expert_meta`ï¼›\n",
    "   - expert forward æ—¶é€ expert å¾ªç¯ï¼ˆæ¯æ¬¡è°ƒç”¨ä¸€ä¸ªå° MLPï¼‰ï¼Œåœ¨ä¸“å®¶å¤šã€token å¤šæ—¶ä¼šäº§ç”Ÿå¤§é‡å° kernel launchã€‚ã€æˆ‘å‘ç°[å¾ˆæ—©æœŸçš„Megatron_LM](https://zhuanlan.zhihu.com/p/666653126)ä¹Ÿæ˜¯è¿™ä¹ˆåšçš„ã€‘\n",
    "\n",
    "2. **`all_gather + padding` çš„é€šä¿¡å½¢æ€ï¼ˆé¢å¤–æ‹·è´/å¸¦å®½æµªè´¹ï¼‰**\n",
    "   - all_gather+pad æ–¹æ¡ˆï¼ˆå½“å‰ baselineï¼‰ï¼šä¸ºæ»¡è¶³å½¢çŠ¶ä¸€è‡´å…ˆ padï¼Œå†å…¨é‡æ”¶é›†ã€å† mask è¿‡æ»¤ï¼Œå¯¼è‡´å¤šæ¬¡æ‹·è´ï¼›è´Ÿè½½ä¸å‡æ—¶ pad è†¨èƒ€ï¼Œæœ‰æ•ˆè½½è·å æ¯”ä½ï¼Œé€šä¿¡å¸¦å®½æµªè´¹ã€‚\n",
    "   - è¿™ç±»â€œå¹¿æ’­å¼æ”¶é›†â€æŠŠæ‰€æœ‰ rank çš„æ•°æ®éƒ½æ¬è¿‡æ¥ï¼Œæœ¬è´¨ä¸Šåšäº†å…¨é‡æ‰©æ•£ï¼Œé¢å¤–å¼€é”€é«˜ï¼›Megatron MoE çš„ MoETokenDispatcher-AlltoAll å°±æ˜¯è¿™ç§ baseline å½¢æ€ã€‚\n",
    "   - ä¼˜åŒ–æ–¹å‘ï¼ˆå‚è€ƒ Megatron Flex/DeepEP æ€è·¯ï¼‰ï¼šç”¨çœŸæ­£çš„ all_to_all + split sizes/permuteï¼Œåªå‘ç»™ç›®æ ‡ rankï¼Œé¿å…å¤§è§„æ¨¡ pad å’Œå¹¿æ’­ï¼›åŒæ—¶é’ˆå¯¹å®½ EP å’Œâ€œæœºå†…é€šä¿¡å’Œè·¨æœºé€šä¿¡é€Ÿç‡å·®äº†ä¸€ä¸ªæ•°é‡çº§â€è¿™ä¸€åŸºæœ¬äº‹å®ï¼Œå»åšä¸¤é˜¶æ®µçš„è½¬å‘ï¼Œç”¨è·¨æœºé€šä¿¡+æœºå†…è½¬å‘çš„æ€è·¯å»å®é™…å‡å°‘è·¨æœºé€šä¿¡é‡ã€‚\n",
    "\n",
    "3. **æœªå®ç° token capacity / drop ç­–ç•¥ï¼ˆçœŸå® MoE å¿…éœ€ï¼‰**\n",
    "   - çœŸå® MoE æ¨ç†/è®­ç»ƒé‡Œé€šå¸¸éœ€è¦å¯¹æ¯ä¸ªä¸“å®¶è®¾å®š capacityï¼ˆæ¯ä¸ª expert æœ€å¤šå¤„ç†å¤šå°‘ tokenï¼‰ï¼Œå¦åˆ™æç«¯è·¯ç”±ä¼šå¯¼è‡´ OOM æˆ–ä¸¥é‡çš„å°¾å»¶è¿Ÿï¼›\n",
    "   - å½“å‰å®ç°é‡Œç”¨ `max_recv = max_num_tokens * world_size` ä½œä¸ºç²—ä¸Šç•Œï¼Œè™½ç„¶â€œèƒ½è·‘â€ï¼Œä½†ï¼š\n",
    "     - å†…å­˜å ç”¨åå¤§ï¼›\n",
    "     - ä¸èƒ½è¡¨è¾¾â€œå®¹é‡ä¸è¶³æ—¶å¦‚ä½•å¤„ç†â€ï¼ˆdrop / reroute / overflow bufferï¼‰ï¼›\n",
    "     - è´Ÿè½½æŠ–åŠ¨ä¼šæ›´æ˜æ˜¾ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaf352f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## æœªæ¥ä¼˜åŒ–æ–¹å‘ï¼ˆä»æ˜“åˆ°éš¾ / ä»æ”¶ç›Šåˆ°å·¥ç¨‹é‡ï¼‰\n",
    "\n",
    "ä¸‹é¢è¿™äº›ä¼˜åŒ–æ–¹å‘å¤§å¤šå¯¹åº”ä½ ä»£ç é‡Œå·²ç»æ ‡æ³¨çš„ TODO æˆ–è€…å½“å‰ç»“æ„è‡ªç„¶å»¶ä¼¸ï¼š\n",
    "\n",
    "1. **ä¸“å®¶è®¡ç®—å‘é‡åŒ–ï¼šç”¨ `bmm`/batched GEMM æ›¿æ¢é€ä¸“å®¶å¾ªç¯**\n",
    "   - ç›®æ ‡ï¼šæŠŠåŒä¸€å¼ å¡ä¸Šå¤šä¸ªæœ¬åœ°ä¸“å®¶çš„ MLP è®¡ç®—æ‰“åŒ…æˆæ›´å¤§çš„çŸ©é˜µä¹˜ï¼Œæé«˜ GPU åˆ©ç”¨ç‡ï¼›\n",
    "   - å¸¸è§åšæ³•æ˜¯æŠŠè½æ¡¶åçš„ `expert_x` ç»„ç»‡æˆé€‚åˆ batch matmul çš„å¸ƒå±€ï¼Œå†ä¸€æ¬¡æ€§è·‘å®Œã€‚\n",
    "\n",
    "2. **é€šä¿¡å½¢æ€ä¼˜åŒ–ï¼šä» `all_gather + mask` è¿ç§»åˆ° `all_to_all_single`ï¼ˆå¸¦ split sizesï¼‰**\n",
    "   - ç›®æ ‡ï¼šåªæŠŠâ€œè¯¥å‘ç»™è°çš„â€å‘ç»™è°ï¼Œé¿å…å…¨é‡ gather å’Œå¤§é‡ paddingï¼›\n",
    "   - å…³é”®å·¥ç¨‹ç‚¹ï¼š\n",
    "     - éœ€è¦æ„å»ºæ¯ä¸ª rank çš„ `send_splits/recv_splits`ï¼›\n",
    "     - combine ä¹Ÿåšé€†å‘ all-to-allï¼›\n",
    "     - è¦ç¡®ä¿ autograd å‹å¥½ï¼ˆå¯ä»¥ç”¨ PyTorch åˆ†å¸ƒå¼çš„ autograd-aware ç®—å­æˆ–æ‰‹å†™ autograd Functionï¼‰ã€‚\n",
    "\n",
    "3. **å®ç° capacity ä¸ overflow ç­–ç•¥ï¼ˆMoE å·¥ç¨‹åŒ–å¿…é¡»é¡¹ï¼‰**\n",
    "   - æ¯ä¸ª expert è®¾å®šå®¹é‡ `capacity = ceil(tokens_per_rank * topk / num_experts * capacity_factor)`ï¼›\n",
    "   - å½“æŸä¸ª expert è¶…è¿‡ capacityï¼š\n",
    "     - æ¨ç†ï¼šå¸¸ç”¨ dropï¼Œå¯¹äºæº¢å‡ºtokenï¼Œèµ°æ®‹å·®é“¾æ¥ç›´æ¥è·³è¿‡ä¸“å®¶å±‚çš„è®¡ç®—ï¼›\n",
    "     - è®­ç»ƒï¼šè¿™é‡Œä¸€èˆ¬ä¼šæœ‰load balance aux lossä½¿å¾—è®­ç»ƒæ—¶å€™ï¼Œå°½é‡è´Ÿè½½å‡è¡¡ï¼Œæ¯”å¦‚GShardã€SwitchTransformerï¼›DeepSeekv3ä¹Ÿæå‡ºäº†åŸºäºåŠ æ€§biasçš„non-aux-lossçš„load balanceè®­ç»ƒåŠæ³•ï¼Œåœ¨[DeepSeekv3 Tech Report](https://arxiv.org/pdf/2412.19437)çš„2.1.2ï¼ˆåŸç†ï¼‰å’Œ4.5.2ï¼ˆæ¶ˆèå®éªŒï¼‰ã€‚\n",
    "\n",
    "![deepseek](./images/Practice03DistMoE04.png)\n",
    "\n",
    "4. **æ¨ç†è·¯å¾„ä¸è®­ç»ƒè·¯å¾„åˆ†ç¦»**\n",
    "   - è®­ç»ƒæ›´å…³æ³¨æ¢¯åº¦æ­£ç¡®ä¸æ•°å€¼ç¨³å®šï¼›\n",
    "   - æ¨ç†æ›´å…³æ³¨å»¶è¿Ÿä¸ååï¼š\n",
    "     - å¯ä»¥ç”¨æ›´æ¿€è¿›çš„ kernel èåˆã€æ›´ç´§å‡‘çš„ bufferï¼ˆæ¯”å¦‚æŒ‰ capacity å›ºå®šå¤§å°ï¼‰ã€æ›´å°‘çš„ dtype è½¬æ¢ï¼›\n",
    "     - å¯èƒ½è¿˜ä¼šå¼•å…¥ expert cachingã€prefetchã€overlapï¼ˆé€šä¿¡/è®¡ç®—é‡å ï¼‰ç­‰ç­–ç•¥ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06601409",
   "metadata": {},
   "source": [
    "> è¿™é‡Œç®€å•è¯´ä¸€ä¸‹ï¼Œå‡å¦‚è¦åŠ  capacity è¿™ä¸ªåŠŸèƒ½çš„è¯ï¼Œå¤§æ¦‚æ¶‰åŠåˆ°å“ªäº›æ–‡ä»¶ï¼Œä»¥åŠå¦‚ä½•æ”¹åŠ¨ï¼š\n",
    "\n",
    "* é…ç½®ï¼šåœ¨ MoEConfig åŠ  capacity_per_expertï¼ˆæˆ– capacity_factor æŒ‰å…¬å¼ç®—å®¹é‡ï¼‰ã€‚è®­ç»ƒå‰ç®—å¥½æ¯ä¸ª rank çš„ capacityã€‚\n",
    "* è·¯ç”±/dispatch å‰æ‹¦æˆªï¼šåœ¨ EPMoE.forward é‡Œ top-k ä¹‹åã€è¿›å…¥ dispatch å‰åšå®¹é‡è£å‰ªã€‚ç»´æŠ¤ä¸€ä¸ª per_expert_counterï¼Œè¶…å‡º capacity çš„ token æ ‡è®°ä¸º droppedï¼š\n",
    "  * å•å¡åˆ†æ”¯ï¼ˆworld_size==1ï¼‰é‡Œï¼Œè½æ¡¶ expert_inputs æ—¶è·³è¿‡è¶…é¢ tokenã€‚\n",
    "  * å¤šå¡åˆ†æ”¯ï¼Œè°ƒç”¨ PyTorchAllToAll.dispatch æ—¶ä¼ å…¥å®¹é‡/ä½¿ç”¨ counterï¼ŒåªæŠŠæœªè¶…é¢çš„ token æ”¾è¿› send_tokens/send_metaã€‚\n",
    "* PyTorchAllToAll.dispatchï¼ˆå¤šå¡è·¯å¾„ï¼‰ä¹Ÿè¦æ¥æ”¶ capacity æˆ– per_expert_counterï¼Œåœ¨æ„å»º send_tokens æ—¶è·³è¿‡è¶…é¢ï¼Œå¹¶åœ¨ expert_num_tokens é‡Œåªç»Ÿè®¡ä¿ç•™ä¸‹æ¥çš„ã€‚\n",
    "* combine/residual å¤„ç†ï¼šå¯¹è¢«ä¸¢å¼ƒçš„ tokenï¼Œè¿”å›â€œæ®‹å·®â€å³å¯ï¼šå¯ä»¥æå‰åˆå§‹åŒ– out_tokens = xï¼ˆæˆ– x * 1.0ï¼‰æˆ–è€…å°†tokenå¯¹åº”çš„ä¸“å®¶æƒé‡ç½®0ï¼Œç„¶å combine å åŠ ä¸“å®¶è¾“å‡ºï¼›è¢« drop çš„ token æ²¡æœ‰ meta/weightï¼Œå°±ä¿æŒæ®‹å·®è¾“å‡ºï¼Œç›´æ¥è·³è¿‡ä¸“å®¶å±‚è®¡ç®—ã€‚\n",
    "* æ¨å¯¼å®¹é‡ï¼š`capacity = ceil((tokens_per_rank * topk / num_experts) * capacity_factor)` å¸¸ç”¨ï¼Œé¿å…ç¡¬ç¼–ç ï¼›åœ¨è®­ç»ƒå…¥å£ç®—å‡ºå¹¶å†™å…¥ cfgã€‚\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
